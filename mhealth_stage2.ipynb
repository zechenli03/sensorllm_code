{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_label_continuity(df):\n",
    "    continuity_segments = {}\n",
    "\n",
    "    for subject in df['subject'].unique():\n",
    "        subject_data = df[df['subject'] == subject]\n",
    "        assert subject_data.index[0]==0\n",
    "\n",
    "        for label in subject_data['activity'].unique():\n",
    "            label_data = subject_data[subject_data['activity'] == label]\n",
    "\n",
    "            indices = label_data.index\n",
    "            segments = []\n",
    "            start_idx = indices[0]\n",
    "\n",
    "            for i in range(len(indices) - 1):\n",
    "                if indices[i] + 1 != indices[i + 1]:\n",
    "                    end_idx = indices[i]\n",
    "                    segments.append((start_idx, end_idx))\n",
    "                    start_idx = indices[i + 1]\n",
    "\n",
    "            segments.append((start_idx, indices[-1]))\n",
    "\n",
    "            if segments:\n",
    "                continuity_segments[(subject, label)] = segments\n",
    "\n",
    "    return continuity_segments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_sequences(sequences, window_size, stride):\n",
    "    assert len(sequences[0]) == 15\n",
    "    has_null = any(any(pd.isnull(item) or item == '' for item in sublist) for sublist in sequences) # 输出结果\n",
    "    if has_null:\n",
    "        raise ValueError(\"Has null values\")\n",
    "\n",
    "    segments = []\n",
    "    labels = []\n",
    "\n",
    "    num_complete_segments = (len(sequences) - window_size) // stride + 1\n",
    "\n",
    "    for i in range(num_complete_segments):\n",
    "        start = i * stride\n",
    "        end = start + window_size\n",
    "        segment = sequences[start:end]\n",
    "        assert len(segment) == window_size\n",
    "        segments.append(np.array(segment))\n",
    "        labels.append([start, end-1])\n",
    "\n",
    "    if labels[-1][1] < len(sequences) - 1:\n",
    "        start = len(sequences) - window_size\n",
    "        end = len(sequences)\n",
    "        segment = sequences[start:end]\n",
    "        assert len(segment) == window_size\n",
    "        segments.append(np.array(segment))\n",
    "        labels.append([start, end-1])\n",
    "    assert len(labels) == len(segments)\n",
    "    print(f\"sequence length: {len(sequences)}\\nsegments: {len(segments)}\")\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    print(labels[:5])\n",
    "    print(labels[-5:])\n",
    "    return segments, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activity_map = {\n",
    "    1: 'Standing still (1 min)',\n",
    "    2: 'Sitting and relaxing (1 min)',\n",
    "    3: 'Lying down (1 min)',\n",
    "    4: 'Walking (1 min)',\n",
    "    5: 'Climbing stairs (1 min)',\n",
    "    6: 'Waist bends forward (20x)',\n",
    "    7: 'Frontal elevation of arms (20x)',\n",
    "    8: 'Knees bending (crouching) (20x)',\n",
    "    9: 'Cycling (1 min)',\n",
    "    10: 'Jogging (1 min)',\n",
    "    11: 'Running (1 min)',\n",
    "    12: 'Jump front & back (20x)'\n",
    "}\n",
    "test_id = ['subject1', 'subject3', 'subject6']\n",
    "window_size=100\n",
    "stride=50\n",
    "\n",
    "all_train_segments = []\n",
    "all_test_segments = []\n",
    "all_train_labels = []\n",
    "all_test_labels = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    df = pd.read_csv(f'./MHEALTHDATASET/mHealth_subject{i}.log', header=None, sep='\\t')\n",
    "    # Note: Excluding the ECG data collected with the chest sensor\n",
    "    df = df.loc[:, [0, 1, 2, 5, 6, 7, 8, 9, 10, 14, 15, 16, 17, 18, 19, 23]].rename(columns= {\n",
    "        0: 'acc_ch_x',\n",
    "        1: 'acc_ch_y',\n",
    "        2: 'acc_ch_z',\n",
    "        5: 'acc_la_x',\n",
    "        6: 'acc_la_y',\n",
    "        7: 'acc_la_z',\n",
    "        8: 'gyr_la_x',\n",
    "        9: 'gyr_la_y',\n",
    "        10: 'gyr_la_z',\n",
    "        14: 'acc_rw_x',\n",
    "        15: 'acc_rw_y',\n",
    "        16: 'acc_rw_z',\n",
    "        17: 'gyr_rw_x',\n",
    "        18: 'gyr_rw_y',\n",
    "        19: 'gyr_rw_z',\n",
    "        23: 'activity'\n",
    "    })\n",
    "    df['subject'] = f'subject{i}'\n",
    "    continuity_segments = check_label_continuity(df)\n",
    "    for key, value in continuity_segments.items():\n",
    "        if key[1] == 0: # class != 1\n",
    "            continue\n",
    "        print(f\"Subject: {key[0]} Activity: {key[1]}\")\n",
    "        for segment in value:\n",
    "            # 划分时间序列数据为片段\n",
    "            rows = df.loc[segment[0]:segment[1]]\n",
    "\n",
    "            assert len(rows['subject'].unique()) == 1\n",
    "            assert rows['subject'].unique()[0] == key[0]\n",
    "            assert len(rows['activity'].unique()) == 1, f\"Subject {key[0]}, activity {key[1]} but has {rows['activity'].unique()},  {segment[0]} 到 {segment[1]}\"\n",
    "            assert rows['activity'].unique()[0] == key[1]\n",
    "\n",
    "            subject_activity_df = rows.iloc[:, ~rows.columns.isin(['subject', 'activity'])]\n",
    "            subject_activity_series = subject_activity_df.values.tolist()\n",
    "\n",
    "            if key[0] not in test_id:\n",
    "                segments, labels = split_sequences(subject_activity_series, window_size, stride)\n",
    "                all_train_segments.extend(segments)\n",
    "            else:\n",
    "                segments, labels = split_sequences(subject_activity_series, window_size, stride)\n",
    "                all_test_segments.extend(segments)\n",
    "\n",
    "            for label in labels:\n",
    "                label_dict = {\n",
    "                    \"subject\": key[0],\n",
    "                    \"activity_name\": activity_map[key[1]],\n",
    "                    \"activity\": key[1]-1,\n",
    "                    \"segments\": label\n",
    "                }\n",
    "                if key[0] not in test_id:\n",
    "                    all_train_labels.append(label_dict)\n",
    "                else:\n",
    "                    all_test_labels.append(label_dict)\n",
    "        print(\"------\"*10)\n",
    "\n",
    "print(f\"all_train_segments: {len(all_train_segments)}\")\n",
    "print(f\"all_train_labels: {len(all_train_labels)}\")\n",
    "print(f\"all_test_segments: {len(all_test_segments)}\")\n",
    "print(f\"all_test_labels: {len(all_test_labels)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_path = \"./whole_data\"\n",
    "\n",
    "with open(os.path.join(output_path, 'train', 'mhealth_train_data_stage2.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_train_segments, f)\n",
    "\n",
    "with open(os.path.join(output_path, 'test', 'mhealth_test_data_stage2.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_test_segments, f)\n",
    "\n",
    "with open(os.path.join(output_path, 'train', 'mhealth_train_labels_stage2.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_train_labels, f)\n",
    "\n",
    "with open(os.path.join(output_path, 'test', 'mhealth_test_labels_stage2.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_test_labels, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import json\n",
    "import re\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"trend_synonyms\": {\n",
    "        \"upward\": \"downward\",\n",
    "        \"ascending\": \"descending\",\n",
    "        \"rising\": \"falling\",\n",
    "        \"increasing\": \"decreasing\"\n",
    "    },\n",
    "    \"steady_synonyms\": [\n",
    "        \"steady\",\n",
    "        \"constant\",\n",
    "        \"stable\"\n",
    "    ],\n",
    "    \"gen_summary_1\": [\n",
    "        \"The provided {data_name} are the 2-second time-series recordings of six sensor channels.\",\n",
    "        \"The given {data_name} represent the 2-second time-series data from six sensor channels.\",\n",
    "        \"{data_name} consist of 2-second time-series measurements from six sensor channels.\",\n",
    "        \"{data_name} include 2-second time-series readings from six sensor channels.\",\n",
    "        \"The supplied {data_name} are the 2-second time-series values of six sensor channels.\",\n",
    "        \"The 2-second time-series data of six sensor channels are provided in {data_name}.\",\n",
    "        \"Contained within {data_name} are the 2-second time-series readings of six sensor channels.\",\n",
    "        \"The dataset {data_name} comprises 2-second time-series observations from six sensor channels.\",\n",
    "        \"In {data_name}, you will find 2-second time-series data recorded from six sensor channels.\",\n",
    "        \"{data_name} hold the 2-second time-series measurements from six distinct sensor channels.\",\n",
    "        \"The {data_name} data set includes 2-second time-series readings from six sensors.\",\n",
    "        \"{data_name} captures the 2-second time-series outputs from six sensor channels.\",\n",
    "        \"2-second time-series information from six sensor channels is contained within {data_name}.\",\n",
    "        \"Included in {data_name} are the 2-second time-series sensor readings from six channels.\",\n",
    "        \"The {data_name} file contains 2-second time-series data for six different sensor channels.\",\n",
    "        \"Within {data_name} are 2-second time-series recordings from six sensor channels.\",\n",
    "        \"{data_name} encapsulates 2-second time-series data collected from six sensor channels.\",\n",
    "        \"The 2-second time-series readings from six sensor channels are found in {data_name}.\",\n",
    "        \"The time-series data in {data_name} spans 2 seconds from six sensor channels.\",\n",
    "        \"{data_name} contains 2-second recordings of time-series data from six sensor channels.\",\n",
    "        \"Included in {data_name} are the time-series readings over 2 seconds from six sensors.\",\n",
    "        \"{data_name} comprises 2-second time-series records from six sensor channels.\",\n",
    "        \"The 2-second data from six sensor channels is included in {data_name}.\",\n",
    "        \"You can find 2-second time-series readings from six sensors in {data_name}.\",\n",
    "        \"The {data_name} dataset contains six channels of 2-second time-series data.\",\n",
    "        \"2-second measurements from six sensor channels are provided in {data_name}.\",\n",
    "        \"{data_name} includes 2-second observations from six different sensor channels.\",\n",
    "        \"The {data_name} resource holds 2-second time-series data from six sensor channels.\",\n",
    "        \"{data_name} features 2-second time-series recordings from six sensor channels.\"\n",
    "    ],\n",
    "    \"gen_summary_2\": [\n",
    "        \"First, let's analyze the trend changes in each channel's data:\\n\",\n",
    "        \"To begin with, let's examine the trend variations in the data for each channel:\\n\",\n",
    "        \"Let's start by looking at the trend changes across each channel's data:\\n\",\n",
    "        \"Initially, let's analyze the trend shifts in the data for each channel:\\n\",\n",
    "        \"First of all, let's delve into the trend variations in each channel's data:\\n\",\n",
    "        \"Let's kick off by analyzing how the trends change in each channel's data:\\n\",\n",
    "        \"Starting with an analysis, let's observe the trend changes in each channel's data:\\n\",\n",
    "        \"Let's commence by examining the trend shifts in each channel's data:\\n\",\n",
    "        \"First, let's take a look at the trend alterations in the data for each channel:\\n\",\n",
    "        \"Let's begin by analyzing the trend changes for each channel's data:\\n\",\n",
    "        \"Initially, let's take a closer look at the trend changes in each channel's data:\\n\",\n",
    "        \"To start, let's explore the trend variations in the data from each channel:\\n\",\n",
    "        \"Let's first analyze the trends in each channel's data:\\n\",\n",
    "        \"First, let's examine how the trends vary across each channel's data:\\n\",\n",
    "        \"Let's begin with an analysis of the trend changes in each channel's data:\\n\",\n",
    "        \"Starting off, let's look at the trend changes in each channel's data:\\n\",\n",
    "        \"To kick things off, let's analyze the trend variations in each channel's data:\\n\",\n",
    "        \"First, let's delve into the changes in trends for each channel's data:\\n\",\n",
    "        \"Let's initiate our analysis by looking at the trend changes in each channel's data:\\n\",\n",
    "        \"First up, let's examine the trend changes across each channel's data:\\n\",\n",
    "        \"Let's get started by analyzing the trend shifts in each channel's data:\\n\",\n",
    "        \"At the outset, let's explore the trend changes in the data for each channel:\\n\",\n",
    "        \"First, let's focus on the trend variations in each channel's data:\\n\",\n",
    "        \"To start with, let's examine the trend changes in each channel's data:\\n\",\n",
    "        \"Initially, let's look into the trend changes within each channel's data:\\n\",\n",
    "        \"Let's start our analysis by checking the trend changes in each channel's data:\\n\",\n",
    "        \"To begin, let's analyze the trend modifications in the data of each channel:\\n\",\n",
    "        \"Firstly, let's explore the trend differences across each channel's data:\\n\",\n",
    "        \"Let's begin our examination by looking at the trend changes in each channel's data:\\n\",\n",
    "        \"Let's start by analyzing how the trends in each channel's data change:\\n\",\n",
    "        \"First, let's observe the trend variations in the data for each channel:\\n\",\n",
    "        \"Let's initially delve into the trend changes in each channel's data:\\n\",\n",
    "        \"To begin our analysis, let's look at the trend shifts in each channel's data:\\n\",\n",
    "        \"Let's start off by examining the trend changes in the data for each channel:\\n\"\n",
    "    ],\n",
    "    \"gen_summary_3_2\": [\n",
    "        \"The data exhibits {trend_num} distinct trend.\",\n",
    "        \"Analysis reveals {trend_num} separate trend within the data.\",\n",
    "        \"There is {trend_num} unique trend identified in the data.\",\n",
    "        \"The data outlines {trend_num} pattern.\",\n",
    "        \"{trend_num} varied trend has been observed in the data.\",\n",
    "        \"The input data displays {trend_num} individual trend.\",\n",
    "        \"In the data, {trend_num} distinct movement trend is evident.\",\n",
    "        \"The data delineates {trend_num} unique trend.\",\n",
    "        \"{trend_num} separate trend can be discerned within the data.\",\n",
    "        \"The data shows {trend_num} different trajectory.\",\n",
    "        \"Analysis of the data shows {trend_num} main trend pattern.\",\n",
    "        \"The data highlights {trend_num} significant trend.\",\n",
    "        \"Overall, the data reflects {trend_num} different development trend.\",\n",
    "        \"The data demonstrates {trend_num} trend type.\",\n",
    "        \"Examining the data, we notice {trend_num} clear trend characteristic.\",\n",
    "        \"The data mirrors {trend_num} different development tendency.\",\n",
    "        \"From a holistic perspective, the data presents {trend_num} unique trend form.\",\n",
    "        \"The data indicates {trend_num} primary shifting trend.\",\n",
    "        \"Parsing through the data, we discover {trend_num} distinct trend feature.\",\n",
    "        \"There is {trend_num} unique trend observed in the data.\",\n",
    "        \"The data shows {trend_num} different trend.\",\n",
    "        \"Analysis reveals {trend_num} separate trend.\",\n",
    "        \"We identified {trend_num} distinct pattern.\",\n",
    "        \"The input data demonstrates {trend_num} unique trend.\",\n",
    "        \"Observation indicates {trend_num} different trend.\",\n",
    "        \"The analysis points to {trend_num} distinct trend.\",\n",
    "        \"{trend_num} separate trend is seen in the data.\",\n",
    "        \"The data reveals {trend_num} distinct trend.\",\n",
    "        \"The data displays {trend_num} individual trend.\",\n",
    "        \"{trend_num} trend is observed in the input data.\",\n",
    "        \"The data contains {trend_num} trend.\",\n",
    "        \"{trend_num} trend is present in the data.\"\n",
    "    ],\n",
    "    \"gen_summary_3\": [\n",
    "        \"The data exhibits {trend_num} distinct trends, with a total of {change_num} changes in trend observed.\",\n",
    "        \"Analysis reveals {trend_num} separate trends within the data, undergoing a cumulative total of {change_num} shifts in direction.\",\n",
    "        \"There are {trend_num} unique trends identified in the data, which altogether have shifted direction {change_num} times.\",\n",
    "        \"The data outlines {trend_num} different patterns, with these patterns changing direction a total of {change_num} times.\",\n",
    "        \"{trend_num} varied trends have been observed in the data, which altogether experienced {change_num} transitions.\",\n",
    "        \"The input data displays {trend_num} individual trends, with a comprehensive change count reaching {change_num}.\",\n",
    "        \"In the data, {trend_num} distinct movement trends are evident, and there have been {change_num} total trend alterations.\",\n",
    "        \"The data delineates {trend_num} unique trends, undergoing {change_num} total changes in these trends.\",\n",
    "        \"{trend_num} separate trends can be discerned within the data, with a total of {change_num} instances of trend modification.\",\n",
    "        \"The data shows {trend_num} different trajectories, with these trajectories having changed a total of {change_num} times.\",\n",
    "        \"Analysis of the data shows {trend_num} main trend patterns, and the trend has undergone {change_num} shifts in total.\",\n",
    "        \"The data highlights {trend_num} significant trends, while also indicating that the trend has changed {change_num} times overall.\",\n",
    "        \"Overall, the data reflects {trend_num} different development trends, which have experienced {change_num} changes in total.\",\n",
    "        \"The data demonstrates {trend_num} major trend types, with the trend undergoing {change_num} turning points during the entire period.\",\n",
    "        \"Examining the data, we notice {trend_num} clear trend characteristics, with the trend fluctuating a total of {change_num} times.\",\n",
    "        \"The data mirrors {trend_num} different development tendencies, while also illustrating that the trend has changed {change_num} times in total.\",\n",
    "        \"From a holistic perspective, the data presents {trend_num} unique trend forms, which have undergone {change_num} changes throughout the process.\",\n",
    "        \"The data indicates {trend_num} primary shifting trends, with these trends transforming a total of {change_num} times.\",\n",
    "        \"Parsing through the data, we discover {trend_num} distinct trend features, with the trend varying {change_num} times over the entire period.\",\n",
    "        \"There are {trend_num} unique trends and {change_num} total trend changes observed in the data.\",\n",
    "        \"The data shows {trend_num} different trends, with {change_num} changes in these trends.\",\n",
    "        \"Analysis reveals {trend_num} separate trends and a total of {change_num} shifts in trend direction.\",\n",
    "        \"We identified {trend_num} distinct patterns, along with {change_num} overall changes in trends in the given data.\",\n",
    "        \"The input data demonstrates {trend_num} unique trends, experiencing {change_num} trend alterations in total.\",\n",
    "        \"Observation indicates {trend_num} different trends with {change_num} instances of trend changes.\",\n",
    "        \"The analysis points to {trend_num} distinct trends and {change_num} changes in the trends.\",\n",
    "        \"{trend_num} separate trends and {change_num} trend shifts are seen in the data.\",\n",
    "        \"The data reveals {trend_num} distinct trends with {change_num} trend variations.\",\n",
    "        \"The data displays {trend_num} individual trends and {change_num} trend fluctuations.\",\n",
    "        \"{change_num} trend changes are observed across {trend_num} trends in the input data.\",\n",
    "        \"The data contains {trend_num} trends, exhibiting {change_num} trend modifications.\",\n",
    "        \"{trend_num} trends are present in the data, with {change_num} instances of trend changes.\",\n",
    "        \"Across {trend_num} trends, the data shows {change_num} occurrences of trend shifts.\"\n",
    "    ],\n",
    "    \"gen_summary_4\": [\n",
    "        \"To sum up, the data exhibited a {trend_type} trend for a cumulative period of {total_time} seconds\",\n",
    "        \"In conclusion, the overall timespan of the data's {trend_type} tendency amounted to {total_time} seconds\",\n",
    "        \"Summarizing the findings, the aggregate time during which the data displayed a {trend_type} pattern was {total_time} seconds\",\n",
    "        \"The analysis reveals that the data's {trend_type} inclination persisted for a total of {total_time} seconds\",\n",
    "        \"To encapsulate, the data's {trend_type} trend spanned a combined duration of {total_time} seconds\",\n",
    "        \"In summary, the data's {trend_type} behavior lasted for an accumulated time of {total_time} seconds\",\n",
    "        \"Recapitulating, the data's {trend_type} tendency endured for an aggregate timeframe of {total_time} seconds\",\n",
    "        \"The investigation concludes that the data's {trend_type} trend had a total lifespan of {total_time} seconds\",\n",
    "        \"To epitomize, the data's {trend_type} characteristic persevered for a sum of {total_time} seconds\",\n",
    "        \"Encapsulating the outcomes, the data's {trend_type} trend stretched across a total time of {total_time} seconds\",\n",
    "        \"In a nutshell, the data's {trend_type} propensity persisted for an accumulated duration of {total_time} seconds\",\n",
    "        \"Summarizing the results, the data's {trend_type} tendency spanned a total timeframe of {total_time} seconds\",\n",
    "        \"The examination reveals that the data's {trend_type} inclination endured for an aggregate of {total_time} seconds\",\n",
    "        \"To encapsulate the findings, the data's {trend_type} behavior lasted for a cumulative period of {total_time} seconds\",\n",
    "        \"In essence, the data exhibited a {trend_type} pattern for a combined time of {total_time} seconds\",\n",
    "        \"The analysis concludes that the data's {trend_type} trend had a total lifespan of {total_time} seconds\",\n",
    "        \"In summary, the data displayed a {trend_type} behavior for an aggregate time of {total_time} seconds\",\n",
    "        \"Overall, the data showed a {trend_type} trend over {total_time} seconds\",\n",
    "        \"In summary, a {trend_type} trend was observed across the span of {total_time} seconds\",\n",
    "        \"To conclude, the trend was {trend_type} over a period of {total_time} seconds\",\n",
    "        \"Summarizing, there was a {trend_type} trend throughout {total_time} seconds\",\n",
    "        \"Briefly, the data trended {trend_type} over the duration of {total_time} seconds\",\n",
    "        \"In total, the data showed a {trend_type} trend lasting {total_time} seconds\",\n",
    "        \"Concisely, the trend observed was {trend_type} for {total_time} seconds\",\n",
    "        \"The input data exhibited a {trend_type} trend during the {total_time} second period\",\n",
    "        \"Upon review, the data's trend was {trend_type} throughout the {total_time} seconds\",\n",
    "        \"The analysis highlighted a {trend_type} trend over the span of {total_time} seconds\",\n",
    "        \"Summarily, a {trend_type} direction was evident across {total_time} seconds of data\"\n",
    "    ],\n",
    "    \"gen_summary_5\": [\n",
    "        \"a {trend_type} pattern for {total_time} seconds\",\n",
    "        \"a {trend_type} trend for {total_time} seconds\",\n",
    "        \"a {trend_type} pattern for a total of {total_time} seconds\",\n",
    "        \"a {trend_type} trend for a total of {total_time} seconds\",\n",
    "        \"a {trend_type} pattern for a sum of {total_time} seconds\",\n",
    "        \"a {trend_type} trend for a sum of {total_time} seconds\",\n",
    "        \"a {trend_type} pattern for a cumulative period of {total_time} seconds\",\n",
    "        \"a {trend_type} trend for a cumulative period of {total_time} seconds\",\n",
    "        \"a {trend_type} pattern for an accumulated time of {total_time} seconds\",\n",
    "        \"a {trend_type} trend for an accumulated time of {total_time} seconds\",\n",
    "        \"a {trend_type} pattern for an aggregate time of {total_time} seconds\",\n",
    "        \"a {trend_type} trend for an aggregate time of {total_time} seconds\",\n",
    "        \"a {trend_type} pattern for {total_time} seconds in total\",\n",
    "        \"a {trend_type} trend for {total_time} seconds in total\",\n",
    "        \"a pattern of {trend_type} for {total_time} seconds\",\n",
    "        \"a trend of {trend_type} for {total_time} seconds\",\n",
    "        \"a {trend_type} trend observed over {total_time} seconds\",\n",
    "        \"a {trend_type} pattern observed over {total_time} seconds\",\n",
    "        \"a {trend_type} trend within a span of {total_time} seconds\",\n",
    "        \"a {trend_type} pattern within a span of {total_time} seconds\",\n",
    "        \"a sequence of {trend_type} occurring over {total_time} seconds\"\n",
    "    ],\n",
    "    \"gen_summary_6\": [\n",
    "        \"The overall trend is {overall_trend}.\",\n",
    "        \"The general trend observed is {overall_trend}.\",\n",
    "        \"Overall, the trend is {overall_trend}.\",\n",
    "        \"The primary trend detected is {overall_trend}.\",\n",
    "        \"In summary, the overall trend is {overall_trend}.\",\n",
    "        \"The main direction we're seeing is {overall_trend}.\",\n",
    "        \"The overarching trend is identified as {overall_trend}.\",\n",
    "        \"Key observation: the overall trend is {overall_trend}.\",\n",
    "        \"The general trend shows {overall_trend}.\",\n",
    "        \"According to the analysis, the overall trend is {overall_trend}.\",\n",
    "        \"The data reveals a {overall_trend} trend in general.\",\n",
    "        \"The predominant trend is observed to be {overall_trend}.\",\n",
    "        \"The overarching trend is determined to be {overall_trend}.\",\n",
    "        \"After calculation, the primary trend is identified as {overall_trend}.\",\n",
    "        \"The general trend is {overall_trend}.\",\n",
    "        \"The prevailing trend is {overall_trend}.\",\n",
    "        \"The trend overall is {overall_trend}.\",\n",
    "        \"The dominant trend is {overall_trend}.\",\n",
    "        \"In summary, the trend is {overall_trend}.\",\n",
    "        \"Broadly, the movement is {overall_trend}.\",\n",
    "        \"The main direction is {overall_trend}.\",\n",
    "        \"The overarching trend is characterized as {overall_trend}.\",\n",
    "        \"The trend direction is {overall_trend}.\",\n",
    "        \"Looking at the big picture, the trend is {overall_trend}.\",\n",
    "        \"Trend overview: {overall_trend}.\"\n",
    "    ],\n",
    "    \"conclude\": [\n",
    "        \"Therefore, the human activity represented by the given data should be {activity}.\",\n",
    "        \"Hence, the human activity indicated by the provided data should be {activity}.\",\n",
    "        \"Thus, the human activity shown by the given data is likely to be {activity}.\",\n",
    "        \"As a result, the human activity reflected in the provided data should be {activity}.\",\n",
    "        \"Consequently, the human activity depicted by the given data should be {activity}.\",\n",
    "        \"In conclusion, the human activity represented by the provided data should be {activity}.\",\n",
    "        \"Therefore, the human activity inferred from the given data should be {activity}.\",\n",
    "        \"Thus, the human activity suggested by the given data should be {activity}.\",\n",
    "        \"As a result, the human activity described by the provided data should be {activity}.\",\n",
    "        \"Hence, the human activity reflected by the given data should be {activity}.\",\n",
    "        \"Therefore, it can be concluded that the human activity represented by the given data should be {activity}.\",\n",
    "        \"Thus, the human activity implied by the provided data should be {activity}.\",\n",
    "        \"Consequently, the human activity indicated by the given data should be {activity}.\",\n",
    "        \"In summary, the human activity represented by the provided data should be {activity}.\",\n",
    "        \"Therefore, the human activity portrayed by the given data should be {activity}.\",\n",
    "        \"As a result, the human activity identified from the given data should be {activity}.\",\n",
    "        \"Thus, the human activity manifested by the provided data should be {activity}.\",\n",
    "        \"Hence, the human activity captured by the given data should be {activity}.\",\n",
    "        \"Consequently, the human activity revealed by the given data should be {activity}.\",\n",
    "        \"Therefore, the human activity suggested by the provided data should be {activity}.\",\n",
    "        \"In conclusion, the human activity inferred from the given data should be {activity}.\",\n",
    "        \"Thus, the human activity shown by the provided data should be {activity}.\",\n",
    "        \"As a result, the human activity evidenced by the given data should be {activity}.\",\n",
    "        \"Therefore, the human activity illustrated by the provided data should be {activity}.\",\n",
    "        \"Hence, the human activity indicated by the given data should be {activity}.\"\n",
    "    ]\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "Q_TEMPLATES = [\n",
    "    \"Which human activity does this {data_name} segment, consisting of {channel_num} channels, represent?\",\n",
    "    \"What human activity is captured in this {data_name} segment with {channel_num} channels?\",\n",
    "    \"Which human action is depicted in this {channel_num}-channel {data_name} segment?\",\n",
    "    \"Can you identify the human activity represented in this {channel_num}-channel {data_name} segment?\",\n",
    "    \"What human behavior is showcased in this {data_name} that includes {channel_num} channels?\",\n",
    "    \"Reading this {channel_num}-channel {data_name} segment, what human activity is being performed?\",\n",
    "    \"Analyze this {data_name} containing {channel_num} channels. What human action does it portray?\",\n",
    "    \"Examine this {data_name} segment with {channel_num} channels. Which human activity does it illustrate?\",\n",
    "    \"Looking at this {channel_num}-channel {data_name}, can you determine the human activity it represents?\",\n",
    "    \"What human task is being carried out in this {data_name} that features {channel_num} channels?\",\n",
    "    \"Considering this {data_name} with {channel_num} channels, what human behavior is being demonstrated?\",\n",
    "    \"Analyzing this {data_name} that contains {channel_num} channels, what human action can you discern?\",\n",
    "    \"Based on the {channel_num} channels {data_name}, which human activity is being exhibited?\",\n",
    "    \"Can you identify the human behavior portrayed in this {channel_num}-channel {data_name} segment?\",\n",
    "    \"Considering the {channel_num} channels {data_name} segment, what human task is being performed?\",\n",
    "    \"Examine this {data_name} consisting of {channel_num} channels. What human activity does it showcase?\",\n",
    "    \"Given this {data_name} segment with {channel_num} channels, can you determine the human action it represents?\",\n",
    "    \"Read this {channel_num}-channel {data_name}. Which human behavior is being demonstrated?\",\n",
    "    \"Look at this {data_name} featuring {channel_num} channels. What human activity is being carried out?\",\n",
    "    \"Study this {channel_num}-channel {data_name} segment. Can you identify the human task it portrays?\",\n",
    "    \"What human action is being showcased in this {data_name} segment that includes {channel_num} channels?\",\n",
    "    \"Which human behavior can you discern from this {channel_num}-channel {data_name}?\",\n",
    "    \"Analyzing the {channel_num} channels {data_name}, what human activity is being exhibited?\",\n",
    "    \"Based on this {data_name} consisting of {channel_num} channels, can you identify the human task being performed?\",\n",
    "    \"Considering this {data_name} segment with {channel_num} channels, which human action is being represented?\",\n",
    "    \"Examine the {channel_num} channels {data_name}. What human behavior does it showcase?\",\n",
    "    \"What human activity does this {data_name} segment, with {channel_num} channels, depict?\",\n",
    "    \"Can you identify the human activity represented by this {data_name}, which has {channel_num} channels?\",\n",
    "    \"Please determine the human activity depicted in this {data_name} segment, comprising {channel_num} channels.\",\n",
    "    \"Identify the human activity captured in this {data_name}, made up of {channel_num} channels.\",\n",
    "    \"What is the human activity shown in this {data_name} segment, containing {channel_num} channels?\",\n",
    "    \"What human activity is being depicted in this {data_name}, featuring {channel_num} channels?\",\n",
    "    \"Can you determine the human activity portrayed in this {data_name} segment, with {channel_num} channels?\",\n",
    "    \"Please identify the human activity captured in this {data_name}, including {channel_num} channels.\",\n",
    "    \"Describe the human activity shown in this {data_name} segment, composed of {channel_num} channels.\",\n",
    "    \"What is the human activity being shown in this {data_name}, involving {channel_num} channels?\",\n",
    "    \"Which human activity does this {data_name} segment, recorded across {channel_num} channels, represent?\",\n",
    "    \"Can you discern the human activity depicted in this {data_name}, with {channel_num} channels involved?\",\n",
    "    \"What activity is being portrayed in this {data_name} segment, spanning {channel_num} channels?\",\n",
    "    \"Please determine the human activity captured in this {data_name}, with {channel_num} channels utilized.\",\n",
    "    \"Identify the human activity depicted in this {data_name} segment, across {channel_num} channels.\",\n",
    "    \"What is the human activity shown in this {data_name}, utilizing {channel_num} channels?\",\n",
    "    \"From the {channel_num} channels {data_name}, what human activity can you infer?\",\n",
    "    \"Given the {channel_num} channels segments of the {data_name}, which human behavior is being portrayed?\",\n",
    "    \"Inspect this {data_name} segment that has {channel_num} channels. Can you identify the human action it represents?\",\n",
    "    \"Observing this {channel_num}-channel {data_name}, what human action can you discern?\",\n",
    "    \"Study the {channel_num} channels segments of the {data_name}. Which human activity is being demonstrated?\",\n",
    "    \"Analyzing this {data_name} with {channel_num} channels, what human behavior can you infer?\",\n",
    "    \"Based on the {channel_num} channels {data_name}, can you identify the human action being performed?\",\n",
    "    \"Considering the {channel_num} channels {data_name} segment, what human behavior is being showcased?\",\n",
    "    \"Based on the {channel_num} channels {data_name}, what human activity can you deduce?\",\n",
    "    \"Considering the {channel_num} channels {data_name} segment, what human activity can be inferred?\",\n",
    "    \"Given the {channel_num} channels {data_name}, what human activity can you speculate?\",\n",
    "    \"With the {channel_num} channels {data_name} segment, what human activity can you conclude?\",\n",
    "    \"In light of the {channel_num} channels {data_name}, what human activity can you determine?\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def num_to_words(num):\n",
    "    units = ['', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "    teens = ['ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen',\n",
    "             'nineteen']\n",
    "    tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety']\n",
    "    scales = ['', 'thousand', 'million', 'billion']\n",
    "\n",
    "    if num < 0:\n",
    "        return \"minus \" + num_to_words(abs(num))\n",
    "\n",
    "    if num < 10:\n",
    "        return units[int(num)]\n",
    "\n",
    "    if num < 20:\n",
    "        return teens[int(num) - 10]\n",
    "\n",
    "    if num < 100:\n",
    "        return tens[int(num) // 10] + (\" \" + num_to_words(num % 10) if num % 10 != 0 else \"\")\n",
    "\n",
    "    if num < 1000:\n",
    "        return units[int(num) // 100] + \" hundred\" + (\" \" + num_to_words(num % 100) if num % 100 != 0 else \"\")\n",
    "\n",
    "    for i, scale in enumerate(scales[1:], 1):\n",
    "        if num < 1000 ** (i + 1):\n",
    "            return num_to_words(num // (1000 ** i)) + \" \" + scale + (\n",
    "                \" \" + num_to_words(num % (1000 ** i)) if num % (1000 ** i) != 0 else \"\")\n",
    "\n",
    "\n",
    "def convert_number(num):\n",
    "    if '.' in str(num):\n",
    "        whole, decimal = str(num).split('.')\n",
    "        if decimal == '0':\n",
    "            return num_to_words(int(num))\n",
    "        else:\n",
    "            return num_to_words(int(whole)) + \" point \" + \" \".join([num_to_words(int(digit)) for digit in decimal])\n",
    "    else:\n",
    "        return num_to_words(int(num))\n",
    "\n",
    "\n",
    "def capitalize_first_letter(string):\n",
    "    if len(string) == 0:\n",
    "        return string\n",
    "    else:\n",
    "        return string[0].upper() + string[1:]\n",
    "\n",
    "\n",
    "def check_a_an(sentence):\n",
    "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "    vowels = 'aeiouAEIOU'\n",
    "    corrected_sentence = sentence\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in ['a', 'an', 'A', 'An']:\n",
    "            if i + 1 < len(words):\n",
    "                next_word = words[i + 1]\n",
    "                if words[i] == 'a' and next_word[0] in vowels:\n",
    "                    corrected_sentence = corrected_sentence.replace(f' a {next_word}', f' an {next_word}', 1)\n",
    "                elif words[i] == 'A' and next_word[0] in vowels:\n",
    "                    corrected_sentence = corrected_sentence.replace(f' A {next_word}', f' An {next_word}', 1)\n",
    "                elif words[i] == 'an' and next_word[0] not in vowels:\n",
    "                    corrected_sentence = corrected_sentence.replace(f' an {next_word}', f' a {next_word}', 1)\n",
    "                elif words[i] == 'An' and next_word[0] not in vowels:\n",
    "                    corrected_sentence = corrected_sentence.replace(f' An {next_word}', f' A {next_word}', 1)\n",
    "\n",
    "    return corrected_sentence\n",
    "\n",
    "\n",
    "def analyze_trend(time_series, sample_rate, start_point=0):\n",
    "    \"\"\"\n",
    "    Analyze the trend of a {data}.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series (list): A list of time series data points.\n",
    "    - duration (int): The duration over which the data was collected.\n",
    "    - threshold (float): The minimum difference between values to consider a change in trend.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame with columns: from_time, to_time, from_value, to_value, trend.\n",
    "    \"\"\"\n",
    "    # Calculate the time interval between data points\n",
    "    time_interval = 1 / sample_rate\n",
    "\n",
    "    # Initialize lists to store the analysis results\n",
    "    from_time, to_time, from_value, to_value, trend = [], [], [], [], []\n",
    "\n",
    "    # Analyze the trend between consecutive data points\n",
    "    for i in range(len(time_series) - 1):\n",
    "        start_time = round((start_point + i) * time_interval, 2)\n",
    "        end_time = round((start_point + i + 1) * time_interval, 2)\n",
    "        start_val = time_series[i]\n",
    "        end_val = time_series[i + 1]\n",
    "\n",
    "        # Determine the trend\n",
    "        if start_val == end_val:\n",
    "            trend_type = 'steady'\n",
    "        elif start_val < end_val:\n",
    "            trend_type = 'increase'\n",
    "        else:\n",
    "            trend_type = 'decrease'\n",
    "\n",
    "        # Append the results to the lists\n",
    "        from_time.append(start_time)\n",
    "        to_time.append(end_time)\n",
    "        from_value.append(start_val)\n",
    "        to_value.append(end_val)\n",
    "        trend.append(trend_type)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'from_time': from_time,\n",
    "        'to_time': to_time,\n",
    "        'from_value': from_value,\n",
    "        'to_value': to_value,\n",
    "        'trend': trend\n",
    "    })\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def merge_adjacent_rows(df):\n",
    "    \"\"\"\n",
    "    Merge adjacent rows with the same trend into a new dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): A DataFrame with columns: from_time, to_time, from_value, to_value, trend.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A merged DataFrame with columns: from_time, to_time, from_value, to_value, trend, values.\n",
    "    \"\"\"\n",
    "    # List to store the merged rows\n",
    "    merged_rows = []\n",
    "\n",
    "    # Variables to store the start of the current segment\n",
    "    current_start_time = df.iloc[0]['from_time']\n",
    "    current_start_value = df.iloc[0]['from_value']\n",
    "    current_trend = df.iloc[0]['trend']\n",
    "    current_values = [current_start_value]\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['trend'] == current_trend:\n",
    "            # Continue accumulating values\n",
    "            current_values.append(row['to_value'])\n",
    "        else:\n",
    "            # Close the current segment and start a new one\n",
    "            merged_rows.append({\n",
    "                'start_time': current_start_time,\n",
    "                'end_time': df.iloc[index - 1]['to_time'],\n",
    "                'start_value': current_start_value,\n",
    "                'end_value': df.iloc[index - 1]['to_value'],\n",
    "                'trend': current_trend,\n",
    "                'values': current_values.copy()\n",
    "            })\n",
    "            current_start_time = row['from_time']\n",
    "            current_start_value = row['from_value']\n",
    "            current_trend = row['trend']\n",
    "            current_values = [current_start_value, row['to_value']]\n",
    "\n",
    "    # Append the last segment\n",
    "    merged_rows.append({\n",
    "        'start_time': current_start_time,\n",
    "        'end_time': df.iloc[-1]['to_time'],\n",
    "        'start_value': current_start_value,\n",
    "        'end_value': df.iloc[-1]['to_value'],\n",
    "        'trend': current_trend,\n",
    "        'values': current_values\n",
    "    })\n",
    "\n",
    "    # Create a DataFrame from the merged rows\n",
    "    merged_df = pd.DataFrame(merged_rows)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def calculate_total_time(df):\n",
    "    \"\"\"\n",
    "    Calculate the total duration for each trend in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): A DataFrame with columns: from_time, to_time, trend.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame with columns: trend, total_time.\n",
    "    \"\"\"\n",
    "    # Group by the trend and sum the duration for each trend\n",
    "    total_time_by_trend = df.groupby('trend').apply(\n",
    "        lambda x: round((x['end_time'] - x['start_time']).sum(), 2)).reset_index(\n",
    "        name='total_time')\n",
    "\n",
    "    return total_time_by_trend\n",
    "\n",
    "def format_floart_2_int(num):\n",
    "    if isinstance(num, float) and num.is_integer():\n",
    "        return int(num)\n",
    "    else:\n",
    "        return num\n",
    "\n",
    "\n",
    "def select_random_pair():\n",
    "    word_pairs = PROMPT_DICT[\"trend_synonyms\"]\n",
    "    upward_word = random.choice(list(word_pairs.keys()))\n",
    "    downward_word = word_pairs[upward_word]\n",
    "    steady_word = random.choice(PROMPT_DICT[\"steady_synonyms\"])\n",
    "    return [upward_word, downward_word, steady_word]\n",
    "\n",
    "\n",
    "def choose_word(input_trend, pair):\n",
    "    if input_trend == \"steady\":\n",
    "        return pair[2]\n",
    "    elif input_trend == \"increase\" or input_trend == \"upward\":\n",
    "        return pair[0]\n",
    "    else:\n",
    "        return pair[1]\n",
    "\n",
    "\n",
    "def choose_decimal_places(std_dev):\n",
    "    if std_dev < 0.01:\n",
    "        return 6\n",
    "    elif std_dev < 0.1:\n",
    "        return 4\n",
    "    elif std_dev < 1:\n",
    "        return 3\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "def generate_correlation_text(correlation_df):\n",
    "    text = \"Pearson Correlation Matrix for each channel:\\n\"\n",
    "    for row in correlation_df.index:\n",
    "        for col in correlation_df.columns:\n",
    "            if row < col:  # 只考虑上三角矩阵的元素\n",
    "                correlation_value = correlation_df.loc[row, col]\n",
    "                if correlation_value >= 0.7:\n",
    "                    correlation_description = \"strongly positively correlated\"\n",
    "                elif correlation_value >= 0.3:\n",
    "                    correlation_description = \"moderately positively correlated\"\n",
    "                elif correlation_value >= 0.1:\n",
    "                    correlation_description = \"weakly positively correlated\"\n",
    "                elif correlation_value <= -0.7:\n",
    "                    correlation_description = \"strongly negatively correlated\"\n",
    "                elif correlation_value <= -0.3:\n",
    "                    correlation_description = \"moderately negatively correlated\"\n",
    "                elif correlation_value <= -0.1:\n",
    "                    correlation_description = \"weakly negatively correlated\"\n",
    "                else:\n",
    "                    correlation_description = \"not significantly correlated\"\n",
    "\n",
    "                text += f\"The correlation between {row} and {col} is {correlation_description}.\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def round_to_8_decimals(number):\n",
    "    return f'{number:.8f}'.rstrip('0').rstrip('.')\n",
    "\n",
    "\n",
    "def gen_reason(d, pair_list, data_type):\n",
    "    assert len(d[0])==15\n",
    "    c_acc_x = d[:, 0]\n",
    "    c_acc_y = d[:, 1]\n",
    "    c_acc_z = d[:, 2]\n",
    "    la_acc_x = d[:, 3]\n",
    "    la_acc_y = d[:, 4]\n",
    "    la_acc_z = d[:, 5]\n",
    "    la_gs_x = d[:, 6]\n",
    "    la_gs_y = d[:, 7]\n",
    "    la_gs_z = d[:, 8]\n",
    "    rla_acc_x = d[:, 9]\n",
    "    rla_acc_y = d[:, 10]\n",
    "    rla_acc_z = d[:, 11]\n",
    "    rla_gs_x = d[:, 12]\n",
    "    rla_gs_y = d[:, 13]\n",
    "    rla_gs_z = d[:, 14]\n",
    "    reading_list = [c_acc_x, c_acc_y, c_acc_z, la_acc_x, la_acc_y, la_acc_z, la_gs_x, la_gs_y, la_gs_z, rla_acc_x, rla_acc_y, rla_acc_z, rla_gs_x, rla_gs_y, rla_gs_z]\n",
    "    reading_name = [\"chest x-axis accelerometer\", \"chest y-axis accelerometer\", \"chest z-axis accelerometer\",\n",
    "                    \"left-ankle x-axis accelerometer\", \"left-ankle y-axis accelerometer\", \"left-ankle z-axis accelerometer\",\n",
    "                    \"left-ankle x-axis gyroscope\", \"left-ankle y-axis gyroscope\", \"left-ankle z-axis gyroscope\",\n",
    "                    \"right-lower-arm x-axis accelerometer\", \"right-lower-arm y-axis accelerometer\", \"right-lower-arm z-axis accelerometer\",\n",
    "                    \"right-lower-arm x-axis gyroscope\", \"right-lower-arm y-axis gyroscope\", \"right-lower-arm z-axis gyroscope\"]\n",
    "\n",
    "    info = {\n",
    "        reading_name[0]: {},\n",
    "        reading_name[1]: {},\n",
    "        reading_name[2]: {},\n",
    "        reading_name[3]: {},\n",
    "        reading_name[4]: {},\n",
    "        reading_name[5]: {},\n",
    "        reading_name[6]: {},\n",
    "        reading_name[7]: {},\n",
    "        reading_name[8]: {},\n",
    "        reading_name[9]: {},\n",
    "        reading_name[10]: {},\n",
    "        reading_name[11]: {},\n",
    "        reading_name[12]: {},\n",
    "        reading_name[13]: {},\n",
    "        reading_name[14]: {}\n",
    "    }\n",
    "\n",
    "    smry_text=[]\n",
    "    trend_text = []\n",
    "    corr_text = []\n",
    "    smry_text.append(\"Statistics for each channel:\\n\")\n",
    "\n",
    "    for r, n in zip(reading_list, reading_name):\n",
    "        data_df = merge_adjacent_rows(analyze_trend(r, sr))\n",
    "        total_time_df = calculate_total_time(data_df)\n",
    "        trend_text.append(n + \": \")\n",
    "\n",
    "        info[n][\"trend_num\"] = len(total_time_df)\n",
    "        info[n][\"total_change_num\"] = len(data_df)\n",
    "\n",
    "        prompts_templates3 = PROMPT_DICT[\"gen_summary_3\"]\n",
    "        prompts_templates3_2 = PROMPT_DICT[\"gen_summary_3_2\"]\n",
    "        prompts_templates4 = PROMPT_DICT[\"gen_summary_4\"]\n",
    "        prompts_templates5 = PROMPT_DICT[\"gen_summary_5\"]\n",
    "        selected_template3_2 = random.choice(prompts_templates3_2)\n",
    "        selected_template3 = random.choice(prompts_templates3)\n",
    "        selected_template4 = random.choice(prompts_templates4)\n",
    "        selected_template5 = random.choice(prompts_templates5)\n",
    "\n",
    "        if info[n][\"trend_num\"] == 1:\n",
    "            trend_text.append(capitalize_first_letter(\n",
    "                selected_template3_2.format(trend_num=random.choice([info[n][\"trend_num\"], convert_number(info[n][\"trend_num\"])]))))\n",
    "        else:\n",
    "            trend_text.append(capitalize_first_letter(\n",
    "                selected_template3.format(trend_num=random.choice([info[n][\"trend_num\"], convert_number(info[n][\"trend_num\"])]),\n",
    "                                          change_num=random.choice([info[n][\"total_change_num\"], convert_number(info[n][\"total_change_num\"])]))))\n",
    "\n",
    "        if 'trend_total_time' not in info[n]:\n",
    "            info[n]['trend_total_time'] = {}\n",
    "\n",
    "        for index, t in total_time_df.iterrows():\n",
    "            info[n][\"trend_total_time\"][t[\"trend\"]] = t['total_time']\n",
    "\n",
    "        i_t = 0\n",
    "        for index, t in total_time_df.iterrows():\n",
    "            if i_t == 0:\n",
    "                if len(total_time_df) == 1:\n",
    "                    trend_text.append(capitalize_first_letter(\n",
    "                        selected_template4.format(trend_type=choose_word(t[\"trend\"], pair_list),\n",
    "                                                  total_time=f\"{t['total_time']:.2f}\")) + \".\")\n",
    "                else:\n",
    "                    trend_text.append(capitalize_first_letter(\n",
    "                        selected_template4.format(trend_type=choose_word(t[\"trend\"], pair_list),\n",
    "                                                  total_time=f\"{t['total_time']:.2f}\")) + \",\")\n",
    "            elif i_t < len(total_time_df) - 1:\n",
    "                trend_text.append(\n",
    "                    selected_template5.format(\n",
    "                        trend_type=choose_word(t[\"trend\"], pair_list),\n",
    "                        total_time=f\"{t['total_time']:.2f}\") + \",\")\n",
    "            else:\n",
    "                trend_text.append(\n",
    "                    \"and \" + selected_template5.format(trend_type=choose_word(t[\"trend\"], pair_list),\n",
    "                                                       total_time=f\"{t['total_time']:.2f}\") + \".\")\n",
    "            i_t += 1\n",
    "\n",
    "        differences = np.diff(r)\n",
    "        sum_of_differences = np.sum(differences)\n",
    "        if sum_of_differences > 0:\n",
    "            info[n][\"overall_trend\"] = \"upward\"\n",
    "        elif sum_of_differences < 0:\n",
    "            info[n][\"overall_trend\"] = \"downward\"\n",
    "        else:\n",
    "            info[n][\"overall_trend\"] = \"steady\"\n",
    "\n",
    "        if info[n][\"total_change_num\"] > 1:\n",
    "            prompts_templates7 = PROMPT_DICT[\"gen_summary_6\"]\n",
    "            selected_template7 = random.choice(prompts_templates7)\n",
    "\n",
    "            trend_text.append(capitalize_first_letter(\n",
    "                selected_template7.format(overall_trend=info[n][\"overall_trend\"]))+'\\n')\n",
    "\n",
    "        info[n][\"min\"] = np.min(r)\n",
    "        info[n][\"max\"] = np.max(r)\n",
    "        info[n][\"median\"] = np.median(r)\n",
    "        info[n][\"mean\"] = np.mean(r)\n",
    "        info[n][\"std_dev\"] = np.std(r)\n",
    "\n",
    "        decimal_places = choose_decimal_places(info[n][\"std_dev\"])\n",
    "        smry_text.append(f\"{n}: Mean={round_to_8_decimals(info[n]['mean'])}, StdDev={round_to_8_decimals(info[n]['std_dev'])}\\n\")\n",
    "\n",
    "        trend_counts = data_df['trend'].value_counts()\n",
    "\n",
    "        if 'trend_total_changes' not in info[n]:\n",
    "            info[n]['trend_total_changes'] = {}\n",
    "\n",
    "        for i_n in range(len(trend_counts)):\n",
    "            info[n][\"trend_total_changes\"][trend_counts.index[i_n]] = trend_counts.values[i_n]\n",
    "\n",
    "    correlation_matrix = np.corrcoef(np.array(reading_list).T, rowvar=False)\n",
    "    correlation_df = pd.DataFrame(correlation_matrix, columns=reading_name, index=reading_name)\n",
    "\n",
    "    corr_text.append(generate_correlation_text(correlation_df))\n",
    "\n",
    "    return {\n",
    "        'smry_text': ' '.join(smry_text),\n",
    "        'trend_text': ' '.join(trend_text),\n",
    "        'corr_text': ' '.join(corr_text)\n",
    "    }\n",
    "\n",
    "\n",
    "def QA_gen(label_dict, data, pair_list):\n",
    "    channel_num = data.shape[1]\n",
    "    prompts_templates = Q_TEMPLATES\n",
    "    selected_template = random.choice(prompts_templates)\n",
    "\n",
    "    data_types = [\"time series data\", \"sensor data\", \"normalized time series data\", \"normalized sensor data\"]\n",
    "    selected_data_type = random.choice(data_types)\n",
    "\n",
    "    c = random.choice([channel_num, convert_number(channel_num)])\n",
    "\n",
    "    question = selected_template.format(data_name=selected_data_type, channel_num=c)\n",
    "    reason = gen_reason(data, pair_list, selected_data_type)\n",
    "\n",
    "    gt = ACTIVITIES[int(label_dict['activity'])]\n",
    "\n",
    "    return {\n",
    "        \"Q\": question,\n",
    "        \"smry\": reason['smry_text'],\n",
    "        \"trend_text\": reason['trend_text'],\n",
    "        \"corr_text\": reason['corr_text'],\n",
    "        \"A\": gt\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ACTIVITIES = {\n",
    "    0: 'Standing still (1 min)',\n",
    "    1: 'Sitting and relaxing (1 min)',\n",
    "    2: 'Lying down (1 min)',\n",
    "    3: 'Walking (1 min)',\n",
    "    4: 'Climbing stairs (1 min)',\n",
    "    5: 'Waist bends forward (20x)',\n",
    "    6: 'Frontal elevation of arms (20x)',\n",
    "    7: 'Knees bending (crouching) (20x)',\n",
    "    8: 'Cycling (1 min)',\n",
    "    9: 'Jogging (1 min)',\n",
    "    10: 'Running (1 min)',\n",
    "    11: 'Jump front & back (20x)'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sr = 50"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_path = './whole_data'\n",
    "split = \"train\"\n",
    "\n",
    "output_path = os.path.join(output_path, split)\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    print(f\"Directory '{output_path}' has created.\")\n",
    "else:\n",
    "    print(f\"Directory '{output_path}' exists.\")\n",
    "\n",
    "qa_dict = {\n",
    "        \"author\": \"\",\n",
    "        \"version\": \"\",\n",
    "        \"date\": str(datetime.now().date()),\n",
    "        \"dataset\": []\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for idx, (l, d) in enumerate(zip(all_train_labels, all_train_segments)):\n",
    "    assert d.shape[0] == 100\n",
    "    assert d.shape[1] == 15\n",
    "    trend_pair_list = select_random_pair()\n",
    "    data_dict = {\n",
    "        \"index\": idx,\n",
    "        \"qa_pair\": QA_gen(l, d, trend_pair_list)\n",
    "    }\n",
    "    qa_dict[\"dataset\"].append(data_dict)\n",
    "    if idx < 10:\n",
    "        print(data_dict[\"qa_pair\"])\n",
    "    idx += 1\n",
    "    print(f\"{idx} finished\")\n",
    "with open(os.path.join(output_path, f\"mhealth_{split}_qa_stage2_cls.json\"), 'w') as f:\n",
    "    json.dump(qa_dict, f, indent=2)\n",
    "print(len(qa_dict[\"dataset\"]), len(all_train_labels))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_path = './whole_data'\n",
    "split = \"test\"\n",
    "\n",
    "output_path = os.path.join(output_path, split)\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    print(f\"Directory '{output_path}' has created.\")\n",
    "else:\n",
    "    print(f\"Directory '{output_path}' exists.\")\n",
    "\n",
    "qa_dict = {\n",
    "        \"author\": \"\",\n",
    "        \"version\": \"\",\n",
    "        \"date\": str(datetime.now().date()),\n",
    "        \"dataset\": []\n",
    "    }\n",
    "\n",
    "for idx, (l, d) in enumerate(zip(all_test_labels, all_test_segments)):\n",
    "    assert d.shape[0] == 100\n",
    "    assert d.shape[1] == 15\n",
    "    trend_pair_list = select_random_pair()\n",
    "    data_dict = {\n",
    "        \"index\": idx,\n",
    "        \"qa_pair\": QA_gen(l, d, trend_pair_list)\n",
    "    }\n",
    "    qa_dict[\"dataset\"].append(data_dict)\n",
    "    if idx < 10:\n",
    "        print(data_dict[\"qa_pair\"])\n",
    "    idx += 1\n",
    "    print(f\"{idx} finished\")\n",
    "with open(os.path.join(output_path, f\"mhealth_{split}_qa_stage2_cls.json\"), 'w') as f:\n",
    "    json.dump(qa_dict, f, indent=2)\n",
    "print(len(qa_dict[\"dataset\"]), len(all_test_labels))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
