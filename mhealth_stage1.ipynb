{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_label_continuity(df):\n",
    "    continuity_segments = {}\n",
    "\n",
    "    for subject in df['subject'].unique():\n",
    "        subject_data = df[df['subject'] == subject]\n",
    "        assert subject_data.index[0]==0\n",
    "\n",
    "        for label in subject_data['activity'].unique():\n",
    "            label_data = subject_data[subject_data['activity'] == label]\n",
    "\n",
    "            indices = label_data.index\n",
    "            segments = []\n",
    "            start_idx = indices[0]\n",
    "\n",
    "            for i in range(len(indices) - 1):\n",
    "                if indices[i] + 1 != indices[i + 1]:\n",
    "                    end_idx = indices[i]\n",
    "                    segments.append((start_idx, end_idx))\n",
    "                    start_idx = indices[i + 1]\n",
    "\n",
    "            segments.append((start_idx, indices[-1]))\n",
    "\n",
    "            if segments:\n",
    "                continuity_segments[(subject, label)] = segments\n",
    "\n",
    "    return continuity_segments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_window_sizes(sequence_length, window_size_list):\n",
    "    min_window_size, max_window_size = window_size_list[0], window_size_list[1]\n",
    "    window_sizes = []\n",
    "    remaining_length = sequence_length\n",
    "    while remaining_length > 0:\n",
    "        if remaining_length < min_window_size:\n",
    "            if window_sizes:\n",
    "                last_window_size = window_sizes.pop()\n",
    "                remaining_length += last_window_size\n",
    "                continue\n",
    "            else:\n",
    "                raise ValueError(\"没有之前生成的窗口大小\")\n",
    "        window_size = random.randint(min_window_size, min(max_window_size, remaining_length))\n",
    "        window_sizes.append(window_size)\n",
    "        remaining_length -= window_size\n",
    "    return window_sizes\n",
    "\n",
    "\n",
    "def split_sequences(sequences, window_size_list, n=1):\n",
    "    assert len(sequences[0]) == 15\n",
    "    # 检查是否存在空值\n",
    "    has_null = any(any(pd.isnull(item) or item == '' for item in sublist) for sublist in sequences)\n",
    "\n",
    "    # 输出结果\n",
    "    if has_null:\n",
    "        raise ValueError(\"Has null values。\")\n",
    "\n",
    "    sequence_length = len(sequences)\n",
    "    segments = []\n",
    "    labels_set = set()\n",
    "    labels = []\n",
    "    for _ in range(n):\n",
    "        window_sizes = generate_window_sizes(sequence_length, window_size_list)\n",
    "\n",
    "        start = 0\n",
    "        for window_size in window_sizes:\n",
    "            end = start + window_size\n",
    "            if (start, end) not in labels_set:\n",
    "                labels_set.add((start, end))\n",
    "                segment = sequences[start:end]\n",
    "                segments.append(np.array(segment))\n",
    "                labels.append([start, end])\n",
    "            start = end\n",
    "    return segments, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activity_map = {\n",
    "    1: 'Standing still (1 min)',\n",
    "    2: 'Sitting and relaxing (1 min)',\n",
    "    3: 'Lying down (1 min)',\n",
    "    4: 'Walking (1 min)',\n",
    "    5: 'Climbing stairs (1 min)',\n",
    "    6: 'Waist bends forward (20x)',\n",
    "    7: 'Frontal elevation of arms (20x)',\n",
    "    8: 'Knees bending (crouching) (20x)',\n",
    "    9: 'Cycling (1 min)',\n",
    "    10: 'Jogging (1 min)',\n",
    "    11: 'Running (1 min)',\n",
    "    12: 'Jump front & back (20x)'\n",
    "}\n",
    "test_id = ['subject1', 'subject3', 'subject6']\n",
    "window_size=[5, 100]\n",
    "\n",
    "all_train_segments = []\n",
    "all_test_segments = []\n",
    "all_train_labels = []\n",
    "all_test_labels = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    df = pd.read_csv(f'./MHEALTHDATASET/mHealth_subject{i}.log', header=None, sep='\\t')\n",
    "    # Note: Excluding the ECG data collected with the chest sensor\n",
    "    df = df.loc[:, [0, 1, 2, 5, 6, 7, 8, 9, 10, 14, 15, 16, 17, 18, 19, 23]].rename(columns= {\n",
    "        0: 'acc_ch_x',\n",
    "        1: 'acc_ch_y',\n",
    "        2: 'acc_ch_z',\n",
    "        5: 'acc_la_x',\n",
    "        6: 'acc_la_y',\n",
    "        7: 'acc_la_z',\n",
    "        8: 'gyr_la_x',\n",
    "        9: 'gyr_la_y',\n",
    "        10: 'gyr_la_z',\n",
    "        14: 'acc_rw_x',\n",
    "        15: 'acc_rw_y',\n",
    "        16: 'acc_rw_z',\n",
    "        17: 'gyr_rw_x',\n",
    "        18: 'gyr_rw_y',\n",
    "        19: 'gyr_rw_z',\n",
    "        23: 'activity'\n",
    "    })\n",
    "    df['subject'] = f'subject{i}'\n",
    "    continuity_segments = check_label_continuity(df)\n",
    "    for key, value in continuity_segments.items():\n",
    "        if key[1] == 0: # class != 1\n",
    "            continue\n",
    "        for segment in value:\n",
    "            # 划分时间序列数据为片段\n",
    "            rows = df.loc[segment[0]:segment[1]]\n",
    "\n",
    "            assert len(rows['subject'].unique()) == 1\n",
    "            assert rows['subject'].unique()[0] == key[0]\n",
    "            assert len(rows['activity'].unique()) == 1, f\"Subject {key[0]}, activity {key[1]} but has {rows['activity'].unique()},  {segment[0]} 到 {segment[1]}\"\n",
    "            assert rows['activity'].unique()[0] == key[1]\n",
    "\n",
    "            subject_activity_df = rows.iloc[:, ~rows.columns.isin(['subject', 'activity'])]\n",
    "            subject_activity_series = subject_activity_df.values.tolist()\n",
    "\n",
    "            if key[0] not in test_id:\n",
    "                segments, labels = split_sequences(subject_activity_series, window_size, 2)\n",
    "                all_train_segments.extend(segments)\n",
    "            else:\n",
    "                segments, labels = split_sequences(subject_activity_series, window_size, 1)\n",
    "                all_test_segments.extend(segments)\n",
    "\n",
    "            for label in labels:\n",
    "                label_dict = {\n",
    "                    \"subject\": key[0],\n",
    "                    \"activity_name\": activity_map[key[1]],\n",
    "                    \"activity\": key[1]-1,\n",
    "                    \"segments\": label\n",
    "                }\n",
    "                if key[0] not in test_id:\n",
    "                    all_train_labels.append(label_dict)\n",
    "                else:\n",
    "                    all_test_labels.append(label_dict)\n",
    "\n",
    "print(f\"all_train_segments: {len(all_train_segments)}\")\n",
    "print(f\"all_train_labels: {len(all_train_labels)}\")\n",
    "print(f\"all_test_segments: {len(all_test_segments)}\")\n",
    "print(f\"all_test_labels: {len(all_test_labels)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_path = \"./data\"\n",
    "\n",
    "with open(os.path.join(output_path, 'train', 'mhealth_train_data_stage1.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_train_segments, f)\n",
    "\n",
    "with open(os.path.join(output_path, 'test', 'mhealth_test_data_stage1.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_test_segments, f)\n",
    "\n",
    "with open(os.path.join(output_path, 'train', 'mhealth_train_labels_stage1.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_train_labels, f)\n",
    "\n",
    "with open(os.path.join(output_path, 'test', 'mhealth_test_labels_stage1.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_test_labels, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(all_test_labels[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"trend_synonyms\": {\n",
    "        \"upward\": \"downward\",\n",
    "        \"ascending\": \"descending\",\n",
    "        \"rising\": \"falling\",\n",
    "        \"increasing\": \"decreasing\",\n",
    "        \"growing\": \"declining\"\n",
    "    },\n",
    "    \"steady_synonyms\": [\n",
    "        \"steady\",\n",
    "        \"constant\",\n",
    "        \"stable\",\n",
    "        \"consistent\"\n",
    "    ],\n",
    "    \"gen_smry_q\": [\n",
    "        \"Could you provide a summary of the main features of the input {data} and the distribution of the trends?\",\n",
    "        \"Please give an overview of the essential attributes of the input {data} and the spread of the trends.\",\n",
    "        \"I would appreciate if you could outline the primary characteristics of the input {data} and the distribution of the trends.\",\n",
    "        \"Can you present a brief description of the fundamental properties of the input {data} and the allocation of the trends?\",\n",
    "        \"Would you be able to summarize the significant aspects of the input {data} and the dispersion of the trends?\",\n",
    "        \"I kindly request a concise report on the central qualities of the input {data} and the distribution of the trends.\",\n",
    "        \"Please provide a succinct account of the crucial elements of the input {data} and the distribution of the trends.\",\n",
    "        \"Please provide a summary of the main features of the input {data} and the trends observed in its distribution.\",\n",
    "        \"Could you analyze the key aspects of the {data} input and outline the distribution trends?\",\n",
    "        \"I need an overview of the primary characteristics of the input {data} and a description of the trend distribution.\",\n",
    "        \"Summarize the essential elements of the input {data} and the patterns in its distribution.\",\n",
    "        \"Explain the fundamental attributes of the {data} input and the distribution trends it exhibits.\",\n",
    "        \"Can you break down the main features and distribution trends of the input {data}?\",\n",
    "        \"Offer a concise summary of the input {data}'s key characteristics and how its trends distribute.\",\n",
    "        \"Detail the core aspects and distribution patterns observed in the {data} input.\",\n",
    "        \"Identify and describe the key features and trend distribution within the input {data}.\",\n",
    "        \"Provide insights into the primary elements and distribution trends of the {data} input.\",\n",
    "        \"Examine the principal attributes of the {data} input and report on the observed distribution trends.\",\n",
    "        \"Highlight the significant characteristics of the input {data} and the nature of its trend distribution.\",\n",
    "        \"Can you summarize the key aspects of {data} and the trend distribution?\",\n",
    "        \"Please outline the primary characteristics of {data} and the trend patterns.\",\n",
    "        \"Could you detail the main features of {data} and outline the trend distribution?\",\n",
    "        \"I need a summary of {data}'s main elements and their trend distributions.\",\n",
    "        \"Please provide insights into the core features of {data} and the distribution of trends.\",\n",
    "        \"Can you highlight the principal components of {data} and their trend distribution?\",\n",
    "        \"Summarize the essential aspects of {data} and the trends' distribution, please.\",\n",
    "        \"Summarize the key features and trend distribution of the {data}.\",\n",
    "        \"What are the main characteristics and trend patterns in the {data}?\",\n",
    "        \"Describe the primary attributes and trend dispersion of the {data}.\",\n",
    "        \"Provide an overview of the {data}'s main features and trend distribution.\",\n",
    "        \"Explain the essential properties and trend spread of the {data}.\",\n",
    "        \"Outline the principal aspects and trend allocation of the {data}.\",\n",
    "        \"Summarize the {data}'s core features and trend dissemination.\",\n",
    "        \"What are the fundamental traits and trend arrangement in the {data}?\",\n",
    "        \"Give a summary of the {data}'s main elements and trend apportionment.\",\n",
    "        \"Describe the salient features and trend distribution within the {data}.\"\n",
    "    ],\n",
    "    \"gen_summary_1\": [\n",
    "        \"The given {data_name} representing the {sensor_name} sensor readings from {start_time}s to {end_time}s.\",\n",
    "        \"The {data_name} represents readings taken from an {sensor_name} sensor between {start_time} and {end_time} seconds.\",\n",
    "        \"This {data_name} comprises {sensor_name} sensor readings collected from {start_time} seconds to {end_time} seconds.\",\n",
    "        \"The {sensor_name} sensor readings recorded within the {start_time} to {end_time} second timeframe are presented in this {data_name}.\",\n",
    "        \"The {data_name} encapsulates {sensor_name} sensor readings from {start_time}s to {end_time}s.\",\n",
    "        \"Readings from an {sensor_name} sensor, captured from {start_time} seconds to {end_time} seconds, are depicted in the given {data_name}.\",\n",
    "        \"The {data_name} illustrates measurements from an {sensor_name} sensor between {start_time} and {end_time} seconds.\",\n",
    "        \"Presented is a span of {data_name}, indicating readings from an {sensor_name} sensor taken within the {start_time} to {end_time} second timeframe.\",\n",
    "        \"This {data_name} reflects the output from an {sensor_name} sensor, measured from {start_time} seconds to {end_time} seconds.\",\n",
    "        \"The presented {data_name} depicts the measurements obtained from an {sensor_name} sensor between {start_time} and {end_time} seconds.\",\n",
    "        \"The {data_name} provided represents the output of an {sensor_name} sensor recorded between {start_time}s and {end_time}s.\",\n",
    "        \"The {data_name} corresponds to the readings collected from an {sensor_name} sensor between {start_time}s and {end_time}s.\",\n",
    "        \"The {data_name} illustrates the {sensor_name} sensor's measurements captured from {start_time}s to {end_time}s.\",\n",
    "        \"The given {data_name} represents the {sensor_name} sensor's output recorded between {start_time} and {end_time} seconds.\",\n",
    "        \"The {data_name} showcases the {sensor_name} sensor's readings acquired between {start_time}s and {end_time}s.\",\n",
    "        \"The {data_name} represent the {sensor_name} sensor's measurements taken from {start_time} seconds to {end_time} seconds.\",\n",
    "        \"The {data_name} encapsulates the {sensor_name} sensor's output collected between {start_time} and {end_time} seconds.\",\n",
    "        \"The {data_name} comprises the {sensor_name} sensor's readings gathered from {start_time}s to {end_time}s.\",\n",
    "        \"The {data_name} exhibits the {sensor_name} sensor's measurements registered within the {start_time} to {end_time} second timeframe.\",\n",
    "        \"The {data_name} displays readings obtained from an {sensor_name} sensor from {start_time} seconds to {end_time} seconds.\",\n",
    "        \"Readings collected from an {sensor_name} sensor from {start_time}s to {end_time}s are documented in this {data_name}.\",\n",
    "        \"This {data_name} encapsulates the readings from an {sensor_name} sensor between {start_time} and {end_time} seconds.\",\n",
    "        \"The provided {data_name} captures the readings from an {sensor_name} sensor, recorded between {start_time} and {end_time} seconds.\",\n",
    "        \"This {data_name} represents the readings from an {sensor_name} sensor between {start_time}s and {end_time}s.\",\n",
    "        \"Readings from an {sensor_name} sensor between {start_time}s and {end_time}s are chronicled in the given {data_name}.\",\n",
    "        \"The {data_name} illustrates {sensor_name} sensor readings between {start_time} and {end_time} seconds.\",\n",
    "        \"Recordings from an {sensor_name} sensor, between {start_time} and {end_time} seconds, are conveyed in this {data_name}.\",\n",
    "        \"The {data_name} provided is a representation of the {sensor_name} sensor's output recorded continuously within the {start_time} to {end_time} second timeframe.\",\n",
    "        \"The presented {data_name} encapsulates the {sensor_name} sensor's readings collected sequentially between {start_time}s and {end_time}s.\",\n",
    "        \"The {data_name} under consideration contains the {sensor_name} sensor's output captured from {start_time} seconds to {end_time} seconds.\",\n",
    "        \"The provided {data_name} shows readings from the {sensor_name} sensor from {start_time}s to {end_time}s.\",\n",
    "        \"The {data_name} contains {sensor_name} sensor data between {start_time}s and {end_time}s.\",\n",
    "        \"Readings from the {sensor_name} sensor between {start_time}s and {end_time}s are in the {data_name}.\",\n",
    "        \"{data_name} includes {sensor_name} sensor observations taken from {start_time} seconds to {end_time} seconds.\",\n",
    "        \"The {data_name} shows {sensor_name} readings from {start_time}s to {end_time}s.\",\n",
    "        \"{sensor_name} sensor data between {start_time}s and {end_time}s is represented in {data_name}.\",\n",
    "        \"{data_name} presents {sensor_name} data collected between {start_time} and {end_time} seconds.\",\n",
    "        \"{sensor_name} readings between {start_time}s and {end_time}s are displayed in {data_name}.\",\n",
    "        \"{sensor_name} sensor readings are captured in {data_name} within the {start_time} to {end_time} second timeframe.\",\n",
    "        \"From {start_time}s to {end_time}s, {sensor_name} data is showcased in the {data_name}.\",\n",
    "        \"The {data_name} exhibits {sensor_name} readings from {start_time} seconds to {end_time} seconds.\"\n",
    "    ],\n",
    "    \"gen_summary_2\": [\n",
    "        \"The data exhibits {trend_num} distinct trends, with a total of {change_num} changes in trend observed.\",\n",
    "        \"Analysis reveals {trend_num} separate trends within the data, undergoing a cumulative total of {change_num} shifts in direction.\",\n",
    "        \"There are {trend_num} unique trends identified in the data, which altogether have shifted direction {change_num} times.\",\n",
    "        \"The data outlines {trend_num} different patterns, with these patterns changing direction a total of {change_num} times.\",\n",
    "        \"{trend_num} varied trends have been observed in the data, which altogether experienced {change_num} transitions.\",\n",
    "        \"The input data displays {trend_num} individual trends, with a comprehensive change count reaching {change_num}.\",\n",
    "        \"In the data, {trend_num} distinct movement trends are evident, and there have been {change_num} total trend alterations.\",\n",
    "        \"The data delineates {trend_num} unique trends, undergoing {change_num} total changes in these trends.\",\n",
    "        \"{trend_num} separate trends can be discerned within the data, with a total of {change_num} instances of trend modification.\",\n",
    "        \"The data shows {trend_num} different trajectories, with these trajectories having changed a total of {change_num} times.\",\n",
    "        \"Analysis of the data shows {trend_num} main trend patterns, and the trend has undergone {change_num} shifts in total.\",\n",
    "        \"The data highlights {trend_num} significant trends, while also indicating that the trend has changed {change_num} times overall.\",\n",
    "        \"Overall, the data reflects {trend_num} different development trends, which have experienced {change_num} changes in total.\",\n",
    "        \"The data demonstrates {trend_num} major trend types, with the trend undergoing {change_num} turning points during the entire period.\",\n",
    "        \"Examining the data, we notice {trend_num} clear trend characteristics, with the trend fluctuating a total of {change_num} times.\",\n",
    "        \"The data mirrors {trend_num} different development tendencies, while also illustrating that the trend has changed {change_num} times in total.\",\n",
    "        \"From a holistic perspective, the data presents {trend_num} unique trend forms, which have undergone {change_num} changes throughout the process.\",\n",
    "        \"The data indicates {trend_num} primary shifting trends, with these trends transforming a total of {change_num} times.\",\n",
    "        \"Parsing through the data, we discover {trend_num} distinct trend features, with the trend varying {change_num} times over the entire period.\",\n",
    "        \"There are {trend_num} unique trends and {change_num} total trend changes observed in the data.\",\n",
    "        \"The data shows {trend_num} different trends, with {change_num} changes in these trends.\",\n",
    "        \"Analysis reveals {trend_num} separate trends and a total of {change_num} shifts in trend direction.\",\n",
    "        \"We identified {trend_num} distinct patterns, along with {change_num} overall changes in trends in the given data.\",\n",
    "        \"The input data demonstrates {trend_num} unique trends, experiencing {change_num} trend alterations in total.\",\n",
    "        \"Observation indicates {trend_num} different trends with {change_num} instances of trend changes.\",\n",
    "        \"The analysis points to {trend_num} distinct trends and {change_num} changes in the trends.\",\n",
    "        \"{trend_num} separate trends and {change_num} trend shifts are seen in the data.\",\n",
    "        \"The data reveals {trend_num} distinct trends with {change_num} trend variations.\",\n",
    "        \"The data displays {trend_num} individual trends and {change_num} trend fluctuations.\",\n",
    "        \"{change_num} trend changes are observed across {trend_num} trends in the input data.\",\n",
    "        \"The data contains {trend_num} trends, exhibiting {change_num} trend modifications.\",\n",
    "        \"{trend_num} trends are present in the data, with {change_num} instances of trend changes.\",\n",
    "        \"Across {trend_num} trends, the data shows {change_num} occurrences of trend shifts.\"\n",
    "    ],\n",
    "    \"gen_summary_2_2\": [\n",
    "        \"The data exhibits {trend_num} distinct trend.\",\n",
    "        \"Analysis reveals {trend_num} separate trend within the data.\",\n",
    "        \"There is {trend_num} unique trend identified in the data.\",\n",
    "        \"The data outlines {trend_num} pattern.\",\n",
    "        \"{trend_num} varied trend has been observed in the data.\",\n",
    "        \"The input data displays {trend_num} individual trend.\",\n",
    "        \"In the data, {trend_num} distinct movement trend is evident.\",\n",
    "        \"The data delineates {trend_num} unique trend.\",\n",
    "        \"{trend_num} separate trend can be discerned within the data.\",\n",
    "        \"The data shows {trend_num} different trajectory.\",\n",
    "        \"Analysis of the data shows {trend_num} main trend pattern.\",\n",
    "        \"The data highlights {trend_num} significant trend.\",\n",
    "        \"Overall, the data reflects {trend_num} different development trend.\",\n",
    "        \"The data demonstrates {trend_num} trend type.\",\n",
    "        \"Examining the data, we notice {trend_num} clear trend characteristic.\",\n",
    "        \"The data mirrors {trend_num} different development tendency.\",\n",
    "        \"From a holistic perspective, the data presents {trend_num} unique trend form.\",\n",
    "        \"The data indicates {trend_num} primary shifting trend.\",\n",
    "        \"Parsing through the data, we discover {trend_num} distinct trend feature.\",\n",
    "        \"There is {trend_num} unique trend observed in the data.\",\n",
    "        \"The data shows {trend_num} different trend.\",\n",
    "        \"Analysis reveals {trend_num} separate trend.\",\n",
    "        \"We identified {trend_num} distinct pattern.\",\n",
    "        \"The input data demonstrates {trend_num} unique trend.\",\n",
    "        \"Observation indicates {trend_num} different trend.\",\n",
    "        \"The analysis points to {trend_num} distinct trend.\",\n",
    "        \"{trend_num} separate trend is seen in the data.\",\n",
    "        \"The data reveals {trend_num} distinct trend.\",\n",
    "        \"The data displays {trend_num} individual trend.\",\n",
    "        \"{trend_num} trend is observed in the input data.\",\n",
    "        \"The data contains {trend_num} trend.\",\n",
    "        \"{trend_num} trend is present in the data.\"\n",
    "    ],\n",
    "    \"gen_summary_3\": [\n",
    "        \"To sum up, the data exhibited a {trend_type} trend for a cumulative period of {total_time} seconds\",\n",
    "        \"In conclusion, the overall timespan of the data's {trend_type} tendency amounted to {total_time} seconds\",\n",
    "        \"Summarizing the findings, the aggregate time during which the data displayed a {trend_type} pattern was {total_time} seconds\",\n",
    "        \"The analysis reveals that the data's {trend_type} inclination persisted for a total of {total_time} seconds\",\n",
    "        \"To encapsulate, the data's {trend_type} trend spanned a combined duration of {total_time} seconds\",\n",
    "        \"In summary, the data's {trend_type} behavior lasted for an accumulated time of {total_time} seconds\",\n",
    "        \"Recapitulating, the data's {trend_type} tendency endured for an aggregate timeframe of {total_time} seconds\",\n",
    "        \"The investigation concludes that the data's {trend_type} trend had a total lifespan of {total_time} seconds\",\n",
    "        \"To epitomize, the data's {trend_type} characteristic persevered for a sum of {total_time} seconds\",\n",
    "        \"Encapsulating the outcomes, the data's {trend_type} trend stretched across a total time of {total_time} seconds\",\n",
    "        \"In a nutshell, the data's {trend_type} propensity persisted for an accumulated duration of {total_time} seconds\",\n",
    "        \"Summarizing the results, the data's {trend_type} tendency spanned a total timeframe of {total_time} seconds\",\n",
    "        \"The examination reveals that the data's {trend_type} inclination endured for an aggregate of {total_time} seconds\",\n",
    "        \"To encapsulate the findings, the data's {trend_type} behavior lasted for a cumulative period of {total_time} seconds\",\n",
    "        \"In essence, the data exhibited a {trend_type} pattern for a combined time of {total_time} seconds\",\n",
    "        \"The analysis concludes that the data's {trend_type} trend had a total lifespan of {total_time} seconds\",\n",
    "        \"In summary, the data displayed a {trend_type} behavior for an aggregate time of {total_time} seconds\",\n",
    "        \"Overall, the data showed a {trend_type} trend over {total_time} seconds\",\n",
    "        \"In summary, a {trend_type} trend was observed across the span of {total_time} seconds\",\n",
    "        \"To conclude, the trend was {trend_type} over a period of {total_time} seconds\",\n",
    "        \"Summarizing, there was a {trend_type} trend throughout {total_time} seconds\",\n",
    "        \"Briefly, the data trended {trend_type} over the duration of {total_time} seconds\",\n",
    "        \"In total, the data showed a {trend_type} trend lasting {total_time} seconds\",\n",
    "        \"Concisely, the trend observed was {trend_type} for {total_time} seconds\",\n",
    "        \"The input data exhibited a {trend_type} trend during the {total_time} second period\",\n",
    "        \"Upon review, the data's trend was {trend_type} throughout the {total_time} seconds\",\n",
    "        \"The analysis highlighted a {trend_type} trend over the span of {total_time} seconds\",\n",
    "        \"Summarily, a {trend_type} direction was evident across {total_time} seconds of data\"\n",
    "    ],\n",
    "    \"gen_summary_4\": [\n",
    "        \"a {trend_type} pattern for {total_time} seconds\",\n",
    "        \"a {trend_type} trend for {total_time} seconds\",\n",
    "        \"a {trend_type} pattern for a total of {total_time} seconds\",\n",
    "        \"a {trend_type} trend for a total of {total_time} seconds\",\n",
    "        \"a {trend_type} pattern for a sum of {total_time} seconds\",\n",
    "        \"a {trend_type} trend for a sum of {total_time} seconds\",\n",
    "        \"a {trend_type} pattern for a cumulative period of {total_time} seconds\",\n",
    "        \"a {trend_type} trend for a cumulative period of {total_time} seconds\",\n",
    "        \"a {trend_type} pattern for an accumulated time of {total_time} seconds\",\n",
    "        \"a {trend_type} trend for an accumulated time of {total_time} seconds\",\n",
    "        \"a {trend_type} pattern for an aggregate time of {total_time} seconds\",\n",
    "        \"a {trend_type} trend for an aggregate time of {total_time} seconds\",\n",
    "        \"a {trend_type} pattern for {total_time} seconds in total\",\n",
    "        \"a {trend_type} trend for {total_time} seconds in total\",\n",
    "        \"a pattern of {trend_type} for {total_time} seconds\",\n",
    "        \"a trend of {trend_type} for {total_time} seconds\",\n",
    "        \"a {trend_type} trend observed over {total_time} seconds\",\n",
    "        \"a {trend_type} pattern observed over {total_time} seconds\",\n",
    "        \"a {trend_type} trend within a span of {total_time} seconds\",\n",
    "        \"a {trend_type} pattern within a span of {total_time} seconds\",\n",
    "        \"a sequence of {trend_type} occurring over {total_time} seconds\"\n",
    "    ],\n",
    "    \"gen_summary_6\": [\n",
    "        \"The overall trend is {overall_trend}.\",\n",
    "        \"The general trend observed is {overall_trend}.\",\n",
    "        \"Overall, the trend is {overall_trend}.\",\n",
    "        \"The primary trend detected is {overall_trend}.\",\n",
    "        \"In summary, the overall trend is {overall_trend}.\",\n",
    "        \"The main direction we're seeing is {overall_trend}.\",\n",
    "        \"The overarching trend is identified as {overall_trend}.\",\n",
    "        \"Key observation: the overall trend is {overall_trend}.\",\n",
    "        \"The general trend shows {overall_trend}.\",\n",
    "        \"According to the analysis, the overall trend is {overall_trend}.\",\n",
    "        \"The data reveals a {overall_trend} trend in general.\",\n",
    "        \"The predominant trend is observed to be {overall_trend}.\",\n",
    "        \"The overarching trend is determined to be {overall_trend}.\",\n",
    "        \"After calculation, the primary trend is identified as {overall_trend}.\",\n",
    "        \"The general trend is {overall_trend}.\",\n",
    "        \"The prevailing trend is {overall_trend}.\",\n",
    "        \"The trend overall is {overall_trend}.\",\n",
    "        \"The dominant trend is {overall_trend}.\",\n",
    "        \"In summary, the trend is {overall_trend}.\",\n",
    "        \"Broadly, the movement is {overall_trend}.\",\n",
    "        \"The main direction is {overall_trend}.\",\n",
    "        \"The overarching trend is characterized as {overall_trend}.\",\n",
    "        \"The trend direction is {overall_trend}.\",\n",
    "        \"Looking at the big picture, the trend is {overall_trend}.\",\n",
    "        \"Trend overview: {overall_trend}.\"\n",
    "    ],\n",
    "    \"gen_trend_q\": [\n",
    "        \"Kindly provide a detailed analysis of the trend changes observed in the {data}.\",\n",
    "        \"Please offer a comprehensive description of how the trends in the {data} have evolved.\",\n",
    "        \"I would appreciate a thorough explanation of the trend fluctuations that occurred within the {data}.\",\n",
    "        \"Could you please examine the {data} in depth and explain the trend shifts observed step by step?\",\n",
    "        \"I kindly request a detailed analysis of the trend changes present in the {data}.\",\n",
    "        \"Please evaluate the {data} trends and provide a detailed description of their development.\",\n",
    "        \"I would be grateful if you could offer a comprehensive account of the trend alterations within the {data}.\",\n",
    "        \"Could you kindly assess the {data} and provide a description of the trend transformations that took place step by step?\",\n",
    "        \"Could you analyze the trends observed in the {data} over the specified period step by step?\",\n",
    "        \"I’m interested in understanding how the {data} has evolved. Could you break down the trend changes for me in detail?\",\n",
    "        \"Please explore the {data} for me, highlighting any significant trends and changes that have occurred.\",\n",
    "        \"I’d appreciate a comprehensive overview of the trends within the {data}, with particular attention to any notable shifts or changes.\",\n",
    "        \"Can you dissect the {data} and explain the trend changes in a detailed manner?\",\n",
    "        \"I'm looking for an in-depth examination of the {data}. Could you elucidate the trends and pivotal changes?\",\n",
    "        \"Please conduct a thorough analysis of the {data}, focusing on the evolution of trends over time.\",\n",
    "        \"Could you delve into the {data} and provide a detailed synopsis of the trends and alterations observed?\",\n",
    "        \"Please analyze the trend shifts in the {data}.\",\n",
    "        \"I need an overview of changes in {data} trends.\",\n",
    "        \"Can you summarize trend fluctuations in the {data}?\",\n",
    "        \"Offer insights into the trend alterations within the {data}.\",\n",
    "        \"Detail the {data}'s trend transitions.\",\n",
    "        \"Examine the evolution of trends in the {data}.\",\n",
    "        \"Explore the changes in trends for the {data}.\",\n",
    "        \"Give a brief analysis of {data} trend developments.\",\n",
    "        \"Please analyze the trend changes in the {data}.\",\n",
    "        \"Describe the trend shifts observed in the {data}.\",\n",
    "        \"Examine how trends in the {data} have evolved.\",\n",
    "        \"What trend changes can be seen in the {data}?\",\n",
    "        \"Identify the trend variations within the {data}.\",\n",
    "        \"Explain the trend developments in the {data}.\",\n",
    "        \"Provide an overview of the trend patterns in the {data}.\",\n",
    "        \"Detail the significant trend modifications in the {data}.\",\n",
    "        \"Analyze the main trend alterations observed in the {data}.\"\n",
    "    ],\n",
    "    \"gen_trend_1\": [\n",
    "        \"Between {start_time} and {end_time} seconds, the data exhibited a {trend} trend.\",\n",
    "        \"The data showed a {trend} trend from {start_time} to {end_time} seconds.\",\n",
    "        \"A {trend} trend was observed in the data spanning {start_time} to {end_time} seconds.\",\n",
    "        \"The time period from {start_time} to {end_time} seconds was characterized by a {trend} trend in the data.\",\n",
    "        \"Over the course of {start_time} to {end_time} seconds, the data displayed a {trend} trend.\",\n",
    "        \"The data followed a {trend} trend during the time frame of {start_time} to {end_time} seconds.\",\n",
    "        \"From {start_time}s to {end_time}s, a {trend} trend was evident in the data.\",\n",
    "        \"The data manifested a {trend} trend within the {start_time} to {end_time} second range.\",\n",
    "        \"Throughout the {start_time} to {end_time} second interval, the data demonstrated a {trend} trend.\",\n",
    "        \"Data analysis from {start_time}s to {end_time}s indicated a {trend} trend.\",\n",
    "        \"In the timeframe from {start_time}s to {end_time}s, the data presented a {trend} trend.\",\n",
    "        \"Observing the data between {start_time} and {end_time} seconds revealed a {trend} trend.\",\n",
    "        \"The data, from {start_time}s to {end_time}s, revealed a {trend} trend.\",\n",
    "        \"From {start_time} to {end_time} seconds, there's a {trend} trend indicated by the data.\",\n",
    "        \"During the period from {start_time}s to {end_time}s, the data exhibited a {trend} trend.\",\n",
    "        \"Between {start_time} and {end_time} seconds, we observed a {trend} trend in the data.\",\n",
    "        \"The data from {start_time} to {end_time} seconds showed a {trend} trend.\",\n",
    "        \"A {trend} trend was evident in the data spanning from {start_time}s to {end_time}s.\",\n",
    "        \"The data displayed a {trend} trend during the period between {start_time}s and {end_time}s.\",\n",
    "        \"Within the {start_time} to {end_time} second range, the data presented a {trend} trend.\",\n",
    "        \"The data revealed a {trend} trend spanning from {start_time} to {end_time} seconds.\",\n",
    "        \"The data showcased a {trend} trend within the timeframe of {start_time}s to {end_time}s.\",\n",
    "        \"Between {start_time} and {end_time} seconds, the data showed a {trend} trend.\",\n",
    "        \"From {start_time} to {end_time} seconds, there was a {trend} trend observed in the data.\",\n",
    "        \"The data demonstrated a {trend} trend from {start_time}s to {end_time}s.\",\n",
    "        \"A {trend} trend in the data was evident between {start_time} and {end_time} seconds.\",\n",
    "        \"Observations from {start_time} to {end_time} seconds indicated a {trend} trend in the data.\",\n",
    "        \"Data between {start_time} seconds and {end_time} seconds revealed a {trend} trend.\",\n",
    "        \"From {start_time}s to {end_time}s, the trend in the data was {trend}.\",\n",
    "        \"The period from {start_time} to {end_time} seconds showed a {trend} trend in the data.\"\n",
    "    ],\n",
    "    \"gen_trend_2\": [\n",
    "        \"Subsequently, a {trend} trend was observed until {end_time}s.\",\n",
    "        \"Following this, the data showed a {trend} trend lasting up to {end_time} seconds.\",\n",
    "        \"Afterward, a {trend} trend emerged and continued until {end_time}s.\",\n",
    "        \"The previous trend was succeeded by a {trend} trend, which persisted up to {end_time} seconds.\",\n",
    "        \"A {trend} trend then followed, extending to {end_time}s.\",\n",
    "        \"The data then exhibited a {trend} trend, which carried on until {end_time} seconds.\",\n",
    "        \"After the previous trend, a {trend} trend was noted, lasting until {end_time}s.\",\n",
    "        \"The subsequent trend was {trend}, and it remained until {end_time} seconds.\",\n",
    "        \"The {trend} trend continued until {end_time} seconds.\",\n",
    "        \"The data maintained a {trend} trend through {end_time}s.\",\n",
    "        \"Up to {end_time} seconds, the data persisted in a {trend} trend.\",\n",
    "        \"The {trend} trend carried on till {end_time} seconds.\",\n",
    "        \"A {trend} trend was sustained by the data up to {end_time} seconds.\",\n",
    "        \"The data upheld a {trend} trend leading up to {end_time}s.\",\n",
    "        \"Until {end_time}s, the data prolonged its {trend} trend.\",\n",
    "        \"The {trend} trend in the data endured up until {end_time} seconds.\",\n",
    "        \"The data perpetuated a {trend} trend all the way to {end_time}s.\",\n",
    "        \"Extending to {end_time}s, the data maintained its {trend} trend.\",\n",
    "        \"This was succeeded by a {trend} trend until {end_time} seconds.\",\n",
    "        \"Subsequently, a {trend} trend was observed up to {end_time} seconds.\",\n",
    "        \"Following this, the data exhibited a {trend} trend up to {end_time}s.\",\n",
    "        \"Thereafter, a {trend} trend continued until {end_time} seconds.\",\n",
    "        \"Afterwards, the trend shifted to {trend} until {end_time} seconds.\",\n",
    "        \"The situation then transitioned into a {trend} trend up to {end_time} seconds.\",\n",
    "        \"Subsequently, the trend moved to {trend} lasting until {end_time} seconds.\",\n",
    "        \"This period was marked by a {trend} trend up until {end_time} seconds.\",\n",
    "        \"Following that period, a {trend} trend was evident until {end_time}s.\",\n",
    "        \"The data then exhibit a {trend} trend until reaching {end_time}s.\",\n",
    "        \"The trend then shifted to a {trend} direction, lasting until {end_time}s.\",\n",
    "        \"What followed was a {trend} trend, extending to {end_time} seconds.\",\n",
    "        \"The data then entered a {trend} phase, which lasted until {end_time} seconds.\",\n",
    "        \"Next, the trend took a {trend} turn, continuing up to {end_time}s.\",\n",
    "        \"This phase was characterized by a {trend} trend until {end_time} seconds.\",\n",
    "        \"It was then that the trend veered towards {trend}, which persisted until {end_time}s.\",\n",
    "        \"Subsequently, the {trend} trend became apparent, prevailing until {end_time} seconds.\",\n",
    "        \"The trend subsequently morphed into a {trend} pattern, holding until {end_time} seconds.\",\n",
    "        \"Following this phase, the trend evolved into a {trend} trajectory until {end_time} seconds.\",\n",
    "        \"Thereafter, the sequence of events led to a {trend} trend, which concluded at {end_time}s.\",\n",
    "        \"Continuing onwards, a {trend} trend was observed through to {end_time} seconds.\",\n",
    "        \"The trend subsequently evolved into a {trend} mode, prevailing up until {end_time} seconds.\",\n",
    "        \"After that, the {trend} trend became the dominant pattern until {end_time} seconds.\",\n",
    "        \"The pattern then entered a {trend} phase, which sustained up to {end_time}s.\",\n",
    "        \"Following this development, the trend solidified into a {trend} direction, continuing until {end_time} seconds.\",\n",
    "        \"The data then aligned with a {trend} trend, which was maintained up to {end_time} seconds.\",\n",
    "        \"Subsequent observations indicated a {trend} trend, lasting until {end_time}s.\",\n",
    "        \"The period following showed a sustained {trend} trend up to {end_time} seconds.\",\n",
    "        \"The subsequent phase was defined by a {trend} trend, enduring until {end_time}s.\",\n",
    "        \"The trend then progressed to a {trend} state, concluding at {end_time} seconds.\",\n",
    "        \"Following this interval, the trend gravitated towards {trend}, persisting through {end_time} seconds.\",\n",
    "        \"Subsequently, the trend shifted into a {trend} trend, which lasted till {end_time} seconds.\",\n",
    "        \"In the next phase, a clear {trend} trend was evident, continuing right up to {end_time}s.\",\n",
    "        \"The data's trajectory shifted towards a {trend} trend, lasting up until {end_time}s.\",\n",
    "        \"After these developments, the {trend} trend took hold, extending to {end_time} seconds.\",\n",
    "        \"The sequence of events led to a {trend} trend, which remained until {end_time}s.\",\n",
    "        \"Then, a {trend} trend until {end_time}s.\",\n",
    "        \"A {trend} trend followed, through {end_time} seconds.\",\n",
    "        \"{trend} trend up to {end_time} seconds.\",\n",
    "        \"Next, {trend} until {end_time}s.\",\n",
    "        \"Followed by {trend} to {end_time}s.\",\n",
    "        \"{trend} persisted until {end_time} seconds.\",\n",
    "        \"Then, {trend} through {end_time} seconds.\",\n",
    "        \"Subsequently, {trend} till {end_time}s.\",\n",
    "        \"{trend} until {end_time} seconds.\",\n",
    "        \"Afterward, {trend} to {end_time}s.\",\n",
    "        \"Continues {trend} until {end_time} seconds.\",\n",
    "        \"Trended {trend} until {end_time}s.\",\n",
    "        \"Showed {trend} until {end_time} seconds.\",\n",
    "        \"Until {end_time} seconds, we observed a {trend} trend.\",\n",
    "        \"As of {end_time}s, the trend was {trend}.\",\n",
    "        \"By {end_time} seconds, there was a noticeable {trend} trend.\",\n",
    "        \"By the time it reached {end_time}s, a {trend} trend was evident.\"\n",
    "    ],\n",
    "    \"gen_trend_3\": [\n",
    "        \"Ultimately, a {trend} trend was seen in the data up until {end_time} seconds.\",\n",
    "        \"The data concluded with a {trend} trend, lasting up to {end_time} seconds.\",\n",
    "        \"The final trend observed in the data was {trend}, which continued up to {end_time} seconds.\",\n",
    "        \"In the end, the data displayed a {trend} trend that lasted up to {end_time}s.\",\n",
    "        \"The concluding trend in the data was {trend}, which persevered up until {end_time}s.\",\n",
    "        \"Up to {end_time}s, the data finished with a {trend} trend.\",\n",
    "        \"The data's final trend was {trend}, which was maintained up to {end_time}s.\",\n",
    "        \"Lastly, a {trend} trend was noted in the data, enduring until {end_time} seconds.\",\n",
    "        \"The data's concluding trend was {trend}, which persisted up to {end_time} seconds.\",\n",
    "        \"At the end, the data exhibited a {trend} trend that carried on until {end_time}s.\",\n",
    "        \"The terminal trend in the data was {trend}, which held fast up to {end_time} seconds.\",\n",
    "        \"In conclusion, the data demonstrated a {trend} trend that continued up to {end_time} seconds.\",\n",
    "        \"Up until {end_time} seconds, the final trend {trend} was noted.\",\n",
    "        \"The final trend had been {trend} up to {end_time}s.\",\n",
    "        \"Continuing until {end_time}s, the trend was decidedly {trend}.\",\n",
    "        \"Ultimately, by {end_time} seconds, a {trend} trend had been observed.\",\n",
    "        \"Conclusively, up to {end_time}s, the data showed a {trend} trend.\",\n",
    "        \"In the end, until {end_time}s, there was an observable {trend} trend.\",\n",
    "        \"To conclude, by {end_time}s, a {trend} trend was noted in the data.\",\n",
    "        \"The observation concluded with a {trend} trend by {end_time}s.\",\n",
    "        \"Finishing at {end_time} seconds, the data revealed a {trend} trend.\",\n",
    "        \"The concluding observation in the data was a {trend} trend, persisting up to {end_time} seconds.\",\n",
    "        \"The data's final chapter was characterized by a {trend} trend, lasting up to {end_time}s.\",\n",
    "        \"The data's end was marked by a {trend} trend, which sustained its direction up to {end_time}s.\",\n",
    "        \"The final data trend, {trend}, stayed its course to {end_time} seconds.\",\n",
    "        \"The data's final trend, {trend}, persisted up to {end_time} seconds.\",\n",
    "        \"The data ended with a {trend} trend up to {end_time}s.\",\n",
    "        \"The data's end saw a {trend} trend, enduring to {end_time} seconds.\",\n",
    "        \"The data's end was {trend}, a trend that held to {end_time} seconds.\"\n",
    "    ],\n",
    "    \"gen_trend_4\": [\n",
    "        \"In summary, the data contains a total of {upward_num} segments with continuous {upward_trend} trends\",\n",
    "        \"Overall, the data shows {upward_num} {upward_trend} trends\",\n",
    "        \"The data reveals a total of {upward_num} segments exhibiting continuous {upward_trend} trends\",\n",
    "        \"Summarizing the data, there are {upward_num} ongoing {upward_trend} trends\",\n",
    "        \"In essence, the dataset comprises {upward_num} segments with persistent {upward_trend} trends\",\n",
    "        \"The summary indicates that the data includes {upward_num} segments with {upward_trend} trends\",\n",
    "        \"In brief, the dataset shows {upward_num} segments with {upward_trend} trends\",\n",
    "        \"To sum up, the data contains {upward_num} uninterrupted {upward_trend} trends\",\n",
    "        \"The analysis indicates that there are {upward_num} segments with continuous {upward_trend} trends\",\n",
    "        \"In short, the data reveals {upward_num} segments with {upward_trend} trends\",\n",
    "        \"According to the data, there are {upward_num} segments showing continuous {upward_trend} trends\",\n",
    "        \"In summary, the analysis found {upward_num} {upward_trend} trends\",\n",
    "        \"The summary reveals {upward_num} segments with {upward_trend} trends\"\n",
    "    ],\n",
    "    \"gen_trend_5\": [\n",
    "        \"{downward_num} segments with continuous {downward_trend} trends\",\n",
    "        \"{downward_num} {downward_trend} trends\",\n",
    "        \"{downward_num} segments exhibiting continuous {downward_trend} trends\",\n",
    "        \"{downward_num} ongoing {downward_trend} trends\",\n",
    "        \"{downward_num} segments with persistent {downward_trend} trends\",\n",
    "        \"{downward_num} segments with {downward_trend} trends\",\n",
    "        \"{downward_num} segments with {downward_trend} trends\",\n",
    "        \"{downward_num} uninterrupted {downward_trend} trends\",\n",
    "        \"{downward_num} segments with continuous {downward_trend} trends\",\n",
    "        \"{downward_num} segments with {downward_trend} trends\",\n",
    "        \"{downward_num} segments showing continuous {downward_trend} trends\",\n",
    "        \"{downward_num} {downward_trend} trends\",\n",
    "        \"{downward_num} segments with {downward_trend} trends\"\n",
    "    ],\n",
    "    \"gen_trend_6\": [\n",
    "        \"and {stable_num} segments with {stable_trend} trends\",\n",
    "        \"and {stable_num} {stable_trend} trends\",\n",
    "        \"and {stable_num} segments exhibiting {stable_trend} trends\",\n",
    "        \"and {stable_num} {stable_trend} trends\",\n",
    "        \"and {stable_num} segments with {stable_trend} trends\",\n",
    "        \"and {stable_num} segments with {stable_trend} trends\",\n",
    "        \"and {stable_num} segments with {stable_trend} trends\",\n",
    "        \"and {stable_num} uninterrupted {stable_trend} trends\",\n",
    "        \"and {stable_num} segments with continuous {stable_trend} trends\",\n",
    "        \"and {stable_num} segments with {stable_trend} trends\",\n",
    "        \"and {stable_num} segments showing {stable_trend} trends\",\n",
    "        \"and {stable_num} {stable_trend} trends\",\n",
    "        \"and {stable_num} segments with {stable_trend} trends\"\n",
    "    ],\n",
    "    \"gen_subtrend_q\": [\n",
    "        \"Please describe how the input {data}'s trends changed from {start_time}s to {end_time}s.\",\n",
    "        \"Kindly analyze the {data} trend variations between {start_time} and {end_time} seconds.\",\n",
    "        \"Please provide an overview of how the {data} trends evolved from {start_time} seconds to {end_time} seconds.\",\n",
    "        \"I would appreciate if you could describe the {data} trend fluctuations that occurred within the {start_time} to {end_time} second timeframe.\",\n",
    "        \"Could you please examine the {data} and explain the trend shifts observed from {start_time}s until {end_time}s?\",\n",
    "        \"I kindly request an analysis of the {data} trend changes spanning the period between {start_time} and {end_time} seconds.\",\n",
    "        \"Please evaluate the {data} trends and describe how they developed from {start_time} seconds to {end_time} seconds.\",\n",
    "        \"I would be grateful if you could provide a description of the {data} trend alterations that took place from {start_time}s to {end_time}s.\",\n",
    "        \"Could you kindly assess the {data} trend transformations occurring within the {start_time} to {end_time} second range?\",\n",
    "        \"Please analyze the trend of the {data} from {start_time} seconds to {end_time} seconds.\",\n",
    "        \"Could you describe how the {data} trend changes between {start_time}s and {end_time}s?\",\n",
    "        \"I'm interested in the input {data}'s trend from {start_time} to {end_time} seconds. Can you break it down for me?\",\n",
    "        \"Can you provide an analysis of the {data} trend from {start_time} seconds to {end_time} seconds?\",\n",
    "        \"Would you be able to detail the trend changes in the given {data} from {start_time} to {end_time} seconds?\",\n",
    "        \"I'd like an overview of how the input {data} evolves between {start_time} and {end_time} seconds. Can you help?\",\n",
    "        \"Please give me an insight into the {data}'s progression from {start_time}s to {end_time}s.\",\n",
    "        \"Could you examine the shift in {data} trends from {start_time} seconds to {end_time} seconds?\",\n",
    "        \"I'm looking for a summary of the {data} trends between {start_time} and {end_time} seconds. What can you tell me?\",\n",
    "        \"Can you explore the changes in the {data}'s trend from {start_time} to {end_time} seconds?\",\n",
    "        \"Describe the trend of {data} from {start_time}s to {end_time}s.\",\n",
    "        \"How did the {data} trend evolve from {start_time} to {end_time} seconds?\",\n",
    "        \"Please outline the change in {data}'s trend between {start_time}s and {end_time}s.\",\n",
    "        \"Can you detail the shift in {data} from {start_time} seconds to {end_time} seconds?\",\n",
    "        \"Explain the transition in {data} from {start_time} to {end_time} seconds.\",\n",
    "        \"Describe the {data}'s trend changes from {start_time} seconds to {end_time} seconds.\",\n",
    "        \"How did the {data}'s trends evolve between {start_time} and {end_time} seconds?\",\n",
    "        \"Explain the {data}'s trend shifts from {start_time}s to {end_time}s.\",\n",
    "        \"Analyze the {data}'s trend variations between {start_time}s and {end_time}s.\",\n",
    "        \"What were the {data}'s trend modifications from {start_time} to {end_time} seconds?\",\n",
    "        \"Summarize the {data}'s trend developments between {start_time} and {end_time} seconds.\",\n",
    "        \"Detail the {data}'s trend fluctuations from {start_time} to {end_time} seconds.\",\n",
    "        \"Examine the {data}'s trend patterns between {start_time}s and {end_time}s.\",\n",
    "        \"Describe how the {data}'s trends altered from {start_time} to {end_time} seconds.\",\n",
    "        \"Provide an overview of the {data}'s trend changes between {start_time} seconds and {end_time} seconds.\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "def capitalize_first_letter(string):\n",
    "    if len(string) == 0:\n",
    "        return string\n",
    "    else:\n",
    "        return string[0].upper() + string[1:]\n",
    "\n",
    "\n",
    "def check_a_an(sentence):\n",
    "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "    vowels = 'aeiouAEIOU'\n",
    "    corrected_sentence = sentence\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in ['a', 'an', 'A', 'An']:\n",
    "            if i + 1 < len(words):\n",
    "                next_word = words[i + 1]\n",
    "                if words[i] == 'a' and next_word[0] in vowels:\n",
    "                    corrected_sentence = corrected_sentence.replace(f' a {next_word}', f' an {next_word}', 1)\n",
    "                elif words[i] == 'A' and next_word[0] in vowels:\n",
    "                    corrected_sentence = corrected_sentence.replace(f' A {next_word}', f' An {next_word}', 1)\n",
    "                elif words[i] == 'an' and next_word[0] not in vowels:\n",
    "                    corrected_sentence = corrected_sentence.replace(f' an {next_word}', f' a {next_word}', 1)\n",
    "                elif words[i] == 'An' and next_word[0] not in vowels:\n",
    "                    corrected_sentence = corrected_sentence.replace(f' An {next_word}', f' A {next_word}', 1)\n",
    "\n",
    "    return corrected_sentence\n",
    "\n",
    "def analyze_trend(time_series, sample_rate, start_point=0):\n",
    "    # Calculate the time interval between data points\n",
    "    time_interval = 1 / sample_rate\n",
    "\n",
    "    # Initialize lists to store the analysis results\n",
    "    from_time, to_time, from_value, to_value, trend = [], [], [], [], []\n",
    "\n",
    "    # Analyze the trend between consecutive data points\n",
    "    for i in range(len(time_series) - 1):\n",
    "        start_time = round((start_point + i) * time_interval, 2)\n",
    "        end_time = round((start_point + i + 1) * time_interval, 2)\n",
    "        start_val = time_series[i]\n",
    "        end_val = time_series[i + 1]\n",
    "\n",
    "        # Determine the trend\n",
    "        if start_val == end_val:\n",
    "            trend_type = 'steady'\n",
    "        elif start_val < end_val:\n",
    "            trend_type = 'increase'\n",
    "        else:\n",
    "            trend_type = 'decrease'\n",
    "\n",
    "        # Append the results to the lists\n",
    "        from_time.append(start_time)\n",
    "        to_time.append(end_time)\n",
    "        from_value.append(start_val)\n",
    "        to_value.append(end_val)\n",
    "        trend.append(trend_type)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'from_time': from_time,\n",
    "        'to_time': to_time,\n",
    "        'from_value': from_value,\n",
    "        'to_value': to_value,\n",
    "        'trend': trend\n",
    "    })\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def merge_adjacent_rows(df):\n",
    "    # List to store the merged rows\n",
    "    merged_rows = []\n",
    "\n",
    "    # Variables to store the start of the current segment\n",
    "    current_start_time = df.iloc[0]['from_time']\n",
    "    current_start_value = df.iloc[0]['from_value']\n",
    "    current_trend = df.iloc[0]['trend']\n",
    "    current_values = [current_start_value]\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['trend'] == current_trend:\n",
    "            # Continue accumulating values\n",
    "            current_values.append(row['to_value'])\n",
    "        else:\n",
    "            # Close the current segment and start a new one\n",
    "            merged_rows.append({\n",
    "                'start_time': current_start_time,\n",
    "                'end_time': df.iloc[index - 1]['to_time'],\n",
    "                'start_value': current_start_value,\n",
    "                'end_value': df.iloc[index - 1]['to_value'],\n",
    "                'trend': current_trend,\n",
    "                'values': current_values.copy()\n",
    "            })\n",
    "            current_start_time = row['from_time']\n",
    "            current_start_value = row['from_value']\n",
    "            current_trend = row['trend']\n",
    "            current_values = [current_start_value, row['to_value']]\n",
    "\n",
    "    # Append the last segment\n",
    "    merged_rows.append({\n",
    "        'start_time': current_start_time,\n",
    "        'end_time': df.iloc[-1]['to_time'],\n",
    "        'start_value': current_start_value,\n",
    "        'end_value': df.iloc[-1]['to_value'],\n",
    "        'trend': current_trend,\n",
    "        'values': current_values\n",
    "    })\n",
    "\n",
    "    # Create a DataFrame from the merged rows\n",
    "    merged_df = pd.DataFrame(merged_rows)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def df2mkd(df):\n",
    "    header = \"| \" + \" | \".join(df.columns) + \" |\"\n",
    "    separator = \"|---\" * len(df.columns) + \"|\"\n",
    "\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        row_str = \"| \" + \" | \".join(str(value) for value in row) + \" |\"\n",
    "        rows.append(row_str)\n",
    "\n",
    "    markdown_table = \"\\n\".join([header, separator] + rows)\n",
    "    return markdown_table\n",
    "\n",
    "\n",
    "def calculate_total_time(df):\n",
    "    \"\"\"\n",
    "    Calculate the total duration for each trend in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): A DataFrame with columns: from_time, to_time, trend.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame with columns: trend, total_time.\n",
    "    \"\"\"\n",
    "    # Group by the trend and sum the duration for each trend\n",
    "    total_time_by_trend = df.groupby('trend').apply(\n",
    "        lambda x: round((x['end_time'] - x['start_time']).sum(), 2)).reset_index(\n",
    "        name='total_time')\n",
    "\n",
    "    return total_time_by_trend\n",
    "\n",
    "\n",
    "def num_to_words(num):\n",
    "    units = ['', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "    teens = ['ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen',\n",
    "             'nineteen']\n",
    "    tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety']\n",
    "    scales = ['', 'thousand', 'million', 'billion']\n",
    "\n",
    "    if num < 0:\n",
    "        return \"minus \" + num_to_words(abs(num))\n",
    "\n",
    "    if num < 10:\n",
    "        return units[int(num)]\n",
    "\n",
    "    if num < 20:\n",
    "        return teens[int(num) - 10]\n",
    "\n",
    "    if num < 100:\n",
    "        return tens[int(num) // 10] + (\" \" + num_to_words(num % 10) if num % 10 != 0 else \"\")\n",
    "\n",
    "    if num < 1000:\n",
    "        return units[int(num) // 100] + \" hundred\" + (\" \" + num_to_words(num % 100) if num % 100 != 0 else \"\")\n",
    "\n",
    "    for i, scale in enumerate(scales[1:], 1):\n",
    "        if num < 1000 ** (i + 1):\n",
    "            return num_to_words(num // (1000 ** i)) + \" \" + scale + (\n",
    "                \" \" + num_to_words(num % (1000 ** i)) if num % (1000 ** i) != 0 else \"\")\n",
    "\n",
    "\n",
    "def convert_number(num):\n",
    "    if '.' in str(num):\n",
    "        whole, decimal = str(num).split('.')\n",
    "        if decimal == '0':\n",
    "            return num_to_words(int(num))\n",
    "        else:\n",
    "            return num_to_words(int(whole)) + \" point \" + \" \".join([num_to_words(int(digit)) for digit in decimal])\n",
    "    else:\n",
    "        return num_to_words(int(num))\n",
    "\n",
    "\n",
    "def format_floart_2_int(num):\n",
    "    if isinstance(num, float) and num.is_integer():\n",
    "        return int(num)\n",
    "    else:\n",
    "        return num\n",
    "\n",
    "\n",
    "def select_random_pair():\n",
    "    word_pairs = PROMPT_DICT[\"trend_synonyms\"]\n",
    "    upward_word = random.choice(list(word_pairs.keys()))\n",
    "    downward_word = word_pairs[upward_word]\n",
    "    steady_word = random.choice(PROMPT_DICT[\"steady_synonyms\"])\n",
    "    return [upward_word, downward_word, steady_word]\n",
    "\n",
    "\n",
    "def choose_word(input_trend, pair):\n",
    "    if input_trend == \"steady\":\n",
    "        return pair[2]\n",
    "    elif input_trend == \"increase\" or input_trend == \"upward\":\n",
    "        return pair[0]\n",
    "    else:\n",
    "        return pair[1]\n",
    "\n",
    "\n",
    "def generate_smry_text(reading, data_df, sensor_type, pair_list):\n",
    "    \"\"\"\n",
    "    Generate a text description of the data.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df (DataFrame): A DataFrame with columns: from_time, to_time, from_value, to_value, trend, values.\n",
    "    - total_time_df (DataFrame): A DataFrame with columns: trend, total_time.\n",
    "\n",
    "    Returns:\n",
    "    - str: A text description of the data.\n",
    "    \"\"\"\n",
    "    total_time_df = calculate_total_time(data_df)\n",
    "    total_time_mkd = df2mkd(total_time_df)\n",
    "    connect_words = [\"followed by \", \"came after \", \"and then \", \"trailed by \", \"which was followed by \",\n",
    "                     \"succeeded by \"]\n",
    "    # Initialize a list to store the text description\n",
    "    prompts_templates1 = PROMPT_DICT[\"gen_summary_1\"]\n",
    "    prompts_templates2 = PROMPT_DICT[\"gen_summary_2\"]\n",
    "    prompts_templates2_2 = PROMPT_DICT[\"gen_summary_2_2\"]\n",
    "    prompts_templates3 = PROMPT_DICT[\"gen_summary_3\"]\n",
    "    prompts_templates4 = PROMPT_DICT[\"gen_summary_4\"]\n",
    "\n",
    "    trend_num = len(total_time_df)\n",
    "    change_num = len(data_df)\n",
    "    data_types = [\"time series data\", \"sensor data\"]\n",
    "\n",
    "    selected_data_type = random.choice(data_types)\n",
    "\n",
    "    selected_template1 = random.choice(prompts_templates1)\n",
    "    selected_template2 = random.choice(prompts_templates2)\n",
    "    selected_template2_2 = random.choice(prompts_templates2_2)\n",
    "    selected_template3 = random.choice(prompts_templates3)\n",
    "    selected_template4 = random.choice(prompts_templates4)\n",
    "    selected_template5 = random.choice(prompts_templates4)\n",
    "\n",
    "    text = []\n",
    "    text.append(capitalize_first_letter(\n",
    "        selected_template1.format(data_name=selected_data_type, sensor_name=sensor_type,\n",
    "                                  start_time=data_df['start_time'].iloc[0], end_time=data_df['end_time'].iloc[-1],)))\n",
    "    if trend_num == 1:\n",
    "        text.append(capitalize_first_letter(\n",
    "            selected_template2_2.format(trend_num=random.choice([trend_num, convert_number(trend_num)]))))\n",
    "    else:\n",
    "        text.append(capitalize_first_letter(\n",
    "            selected_template2.format(trend_num=random.choice([trend_num, convert_number(trend_num)]),\n",
    "                                      change_num=random.choice([change_num, convert_number(change_num)]))))\n",
    "\n",
    "    i_t = 0\n",
    "    for index, t in total_time_df.iterrows():\n",
    "        if i_t == 0:\n",
    "            if len(total_time_df) == 1:\n",
    "                text.append(capitalize_first_letter(\n",
    "                    selected_template3.format(trend_type=choose_word(t[\"trend\"], pair_list),\n",
    "                                              total_time=f\"{t['total_time']:.2f}\")) + \".\")\n",
    "            else:\n",
    "                text.append(capitalize_first_letter(\n",
    "                    selected_template3.format(trend_type=choose_word(t[\"trend\"], pair_list),\n",
    "                                              total_time=f\"{t['total_time']:.2f}\")) + \",\")\n",
    "        elif i_t < len(total_time_df) - 1:\n",
    "            text.append(\n",
    "                random.choice(connect_words) + selected_template4.format(trend_type=choose_word(t[\"trend\"], pair_list),\n",
    "                                                                         total_time=f\"{t['total_time']:.2f}\") + \",\")\n",
    "        else:\n",
    "            text.append(\n",
    "                \"and \" + selected_template5.format(trend_type=choose_word(t[\"trend\"], pair_list),\n",
    "                                                   total_time=f\"{t['total_time']:.2f}\") + \".\")\n",
    "        i_t += 1\n",
    "\n",
    "    differences = np.diff(reading)\n",
    "    sum_of_differences = np.sum(differences)\n",
    "    if sum_of_differences > 0:\n",
    "        overall_trend = choose_word(\"upward\", pair_list)\n",
    "    elif sum_of_differences < 0:\n",
    "        overall_trend = choose_word(\"downward\", pair_list)\n",
    "    else:\n",
    "        overall_trend = choose_word(\"steady\", pair_list)\n",
    "\n",
    "    if change_num > 1:\n",
    "        prompts_templates7 = PROMPT_DICT[\"gen_summary_6\"]\n",
    "        selected_template7 = random.choice(prompts_templates7)\n",
    "\n",
    "        text.append(capitalize_first_letter(\n",
    "            selected_template7.format(overall_trend=overall_trend)))\n",
    "\n",
    "    return check_a_an(' '.join(text)), total_time_mkd\n",
    "\n",
    "\n",
    "def generate_trend_text(data_df, pair_list):\n",
    "    text_detailed = []\n",
    "\n",
    "    prompts_templates1 = PROMPT_DICT[\"gen_trend_1\"]\n",
    "    prompts_templates2 = PROMPT_DICT[\"gen_trend_2\"]\n",
    "    prompts_templates3 = PROMPT_DICT[\"gen_trend_3\"]\n",
    "\n",
    "    i_d = 0\n",
    "    for index, d in data_df.iterrows():\n",
    "        if i_d == 0:\n",
    "            selected_template1 = random.choice(prompts_templates1)\n",
    "            text_detailed.append(capitalize_first_letter(\n",
    "                selected_template1.format(start_time=d['start_time'],\n",
    "                                          end_time=d['end_time'],\n",
    "                                          trend=choose_word(d[\"trend\"],\n",
    "                                                            pair_list))))\n",
    "        elif i_d < len(data_df) - 1:\n",
    "            selected_template2 = random.choice(prompts_templates2)\n",
    "            text_detailed.append(capitalize_first_letter(\n",
    "                selected_template2.format(end_time=d['end_time'],\n",
    "                                          trend=choose_word(d[\"trend\"],\n",
    "                                                            pair_list))))\n",
    "        else:\n",
    "            selected_template3 = random.choice(prompts_templates3)\n",
    "            text_detailed.append(capitalize_first_letter(\n",
    "                selected_template3.format(end_time=d['end_time'],\n",
    "                                          trend=choose_word(d[\"trend\"],\n",
    "                                                            pair_list))))\n",
    "        i_d += 1\n",
    "\n",
    "    prompts_templates4 = PROMPT_DICT[\"gen_trend_4\"]\n",
    "    prompts_templates5 = PROMPT_DICT[\"gen_trend_5\"]\n",
    "    prompts_templates6 = PROMPT_DICT[\"gen_trend_6\"]\n",
    "\n",
    "    rdm_list = list(range(0, len(prompts_templates4)))\n",
    "    selected_num = random.choice(rdm_list)\n",
    "\n",
    "    selected_template4 = prompts_templates4[selected_num]\n",
    "    selected_template5 = prompts_templates5[selected_num]\n",
    "    selected_template6 = prompts_templates6[selected_num]\n",
    "\n",
    "    trend_counts = data_df['trend'].value_counts()\n",
    "    num_trends = len(trend_counts)\n",
    "\n",
    "    for i_n in range(num_trends):\n",
    "        if i_n == 0:\n",
    "            if num_trends > 1:\n",
    "                text_detailed.append(capitalize_first_letter(\n",
    "                    selected_template4.format(upward_num=trend_counts.values[i_n],\n",
    "                                              upward_trend=choose_word(trend_counts.index[i_n], pair_list))) + ',')\n",
    "        elif i_n < num_trends - 1:\n",
    "            text_detailed.append(\n",
    "                selected_template5.format(downward_num=trend_counts.values[i_n],\n",
    "                                          downward_trend=choose_word(trend_counts.index[i_n], pair_list)) + ',')\n",
    "        else:\n",
    "            text_detailed.append(\n",
    "                selected_template6.format(stable_num=trend_counts.values[i_n],\n",
    "                                          stable_trend=choose_word(trend_counts.index[i_n], pair_list)) + '.')\n",
    "\n",
    "    return check_a_an(' '.join(text_detailed))\n",
    "\n",
    "\n",
    "def generate_simple_trend_text(data_df, pair_list):\n",
    "    text_detailed = []\n",
    "\n",
    "    prompts_templates = [\n",
    "        \"{start_time}s to {end_time}s: {trend}\",\n",
    "        \"{start_time} seconds to {end_time} seconds: {trend}\",\n",
    "        \"{start_time} to {end_time} seconds: {trend}\",\n",
    "        \"{start_time}-{end_time} seconds: {trend}\",\n",
    "        \"{start_time}-{end_time}s: {trend}\",\n",
    "        \"{start_time}s-{end_time}s: {trend}\"\n",
    "    ]\n",
    "\n",
    "    prompts_templates_2 = [\n",
    "        \"Number of {trend} trends: {num}\",\n",
    "        \"Count of {trend} trends: {num}\",\n",
    "        \"Total {trend} trends: {num}\",\n",
    "        \"Number of {trend} segments: {num}\",\n",
    "        \"Count of {trend} segments: {num}\",\n",
    "        \"Total {trend} segments: {num}\"\n",
    "    ]\n",
    "\n",
    "    selected_template = random.choice(prompts_templates)\n",
    "    selected_template2 = random.choice(prompts_templates_2)\n",
    "\n",
    "    for index, df in data_df.iterrows():\n",
    "        text_detailed.append(selected_template.format(start_time=df['start_time'],\n",
    "                                                      end_time=df['end_time'],\n",
    "                                                      trend=choose_word(df[\"trend\"],\n",
    "                                                                        pair_list)))\n",
    "\n",
    "    trend_counts = data_df['trend'].value_counts()\n",
    "    num_trends = len(trend_counts)\n",
    "    if num_trends > 1:\n",
    "        for i_n in range(num_trends):\n",
    "            text_detailed.append(selected_template2.format(trend=choose_word(trend_counts.index[i_n], pair_list),\n",
    "                                                           num=trend_counts.values[i_n]))\n",
    "\n",
    "    return check_a_an('\\n'.join(text_detailed))\n",
    "\n",
    "\n",
    "def dscb_trend(df, sensor_type, pair_list, whether_gpt=False, model_type='3.5'):\n",
    "    data_types = [\"time series data\", \"sensor data\"]\n",
    "    prompts_templates = PROMPT_DICT[\"gen_subtrend_q\"]\n",
    "\n",
    "    selected_data_type = random.choice(data_types)\n",
    "    selected_template = random.choice(prompts_templates)\n",
    "\n",
    "    question = selected_template.format(data=selected_data_type,\n",
    "                                        start_time=df[\"start_time\"].iloc[0],\n",
    "                                        end_time=df['end_time'].iloc[-1]\n",
    "                                        )\n",
    "    answer = generate_trend_text(df, pair_list)\n",
    "\n",
    "    return {\n",
    "        \"Q\": question,\n",
    "        \"A\": answer,\n",
    "        \"type\": \"trend\"\n",
    "    }\n",
    "\n",
    "\n",
    "def dscb_simple_trend(df, sensor_type, pair_list, whether_gpt=False, model_type='3.5'):\n",
    "    data_types = [\"time series data\", \"sensor data\"]\n",
    "    prompts_templates = PROMPT_DICT[\"gen_trend_q\"]\n",
    "\n",
    "    selected_data_type = random.choice(data_types)\n",
    "    selected_template = random.choice(prompts_templates)\n",
    "\n",
    "    question = selected_template.format(data=selected_data_type)\n",
    "\n",
    "    answer = generate_simple_trend_text(df, pair_list)\n",
    "\n",
    "    return {\n",
    "        \"Q\": question,\n",
    "        \"A\": answer,\n",
    "        \"type\": \"simple_trend\"\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def QA_summary(reading, trend_df, sensor_type, pair_list, whether_gpt=False, model_type='3.5'):\n",
    "    data_types = [\"time series data\", \"sensor data\"]\n",
    "    prompts_templates = PROMPT_DICT[\"gen_subtrend_q\"]\n",
    "\n",
    "    selected_data_type = random.choice(data_types)\n",
    "    selected_template = random.choice(prompts_templates)\n",
    "\n",
    "    question = selected_template.format(data=selected_data_type,\n",
    "                                        start_time=trend_df[\"start_time\"].iloc[0],\n",
    "                                        end_time=trend_df['end_time'].iloc[-1]\n",
    "                                        )\n",
    "\n",
    "    answer, smry_mkd_df = generate_smry_text(reading, trend_df, sensor_type, pair_list)\n",
    "    return {\n",
    "        \"Q\": question,\n",
    "        \"A\": answer,\n",
    "        \"smry_table\": smry_mkd_df,\n",
    "        \"type\": \"summary\"\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "sr = 50\n",
    "qa_dict = {\"author\": \"\",\n",
    "    \"version\": \"\",\n",
    "    \"date\": str(datetime.now().date()),\n",
    "    \"dataset\": []\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "for d in all_train_segments:\n",
    "    assert len(d[0])==15\n",
    "    c_acc_x = d[:, 0]\n",
    "    c_acc_y = d[:, 1]\n",
    "    c_acc_z = d[:, 2]\n",
    "    la_acc_x = d[:, 3]\n",
    "    la_acc_y = d[:, 4]\n",
    "    la_acc_z = d[:, 5]\n",
    "    la_gs_x = d[:, 6]\n",
    "    la_gs_y = d[:, 7]\n",
    "    la_gs_z = d[:, 8]\n",
    "    rla_acc_x = d[:, 9]\n",
    "    rla_acc_y = d[:, 10]\n",
    "    rla_acc_z = d[:, 11]\n",
    "    rla_gs_x = d[:, 12]\n",
    "    rla_gs_y = d[:, 13]\n",
    "    rla_gs_z = d[:, 14]\n",
    "    reading_list = [c_acc_x, c_acc_y, c_acc_z, la_acc_x, la_acc_y, la_acc_z, la_gs_x, la_gs_y, la_gs_z, rla_acc_x, rla_acc_y, rla_acc_z, rla_gs_x, rla_gs_y, rla_gs_z]\n",
    "    reading_name = [\"chest x-axis accelerometer\", \"chest y-axis accelerometer\", \"chest z-axis accelerometer\",\n",
    "                    \"left-ankle x-axis accelerometer\", \"left-ankle y-axis accelerometer\", \"left-ankle z-axis accelerometer\",\n",
    "                    \"left-ankle x-axis gyroscope\", \"left-ankle y-axis gyroscope\", \"left-ankle z-axis gyroscope\",\n",
    "                    \"right-lower-arm x-axis accelerometer\", \"right-lower-arm y-axis accelerometer\", \"right-lower-arm z-axis accelerometer\",\n",
    "                    \"right-lower-arm x-axis gyroscope\", \"right-lower-arm y-axis gyroscope\", \"right-lower-arm z-axis gyroscope\"]\n",
    "\n",
    "    data_dict = {\n",
    "        \"index\": i,\n",
    "        \"summaries\": {},\n",
    "        \"qa_pairs\": {name: [] for name in reading_name}\n",
    "    }\n",
    "\n",
    "    for r, n in zip(reading_list, reading_name):\n",
    "        normalized_n = \"normalized \" + n\n",
    "        t_df = analyze_trend(r, sr)\n",
    "        trend_dataframe = merge_adjacent_rows(t_df)\n",
    "\n",
    "        trend_pair_list = select_random_pair()\n",
    "\n",
    "        data_dict[\"summaries\"][n] = QA_summary(r, trend_dataframe, normalized_n, trend_pair_list,\n",
    "                                               whether_gpt=False, model_type='3.5')\n",
    "        data_dict[\"qa_pairs\"][n].append(dscb_simple_trend(trend_dataframe, normalized_n, trend_pair_list,\n",
    "                                                          whether_gpt=False, model_type='4'))\n",
    "    qa_dict[\"dataset\"].append(data_dict)\n",
    "    print(f\"{i} finished\")\n",
    "    i += 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "with open(os.path.join(output_path, 'train', f\"mhealth_train_qa_stage1.json\"), 'w') as f:\n",
    "    json.dump(qa_dict, f, indent=2)\n",
    "print(len(qa_dict[\"dataset\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qa_dict = {\"author\": \"\",\n",
    "    \"version\": \"\",\n",
    "    \"date\": str(datetime.now().date()),\n",
    "    \"dataset\": []\n",
    "}\n",
    "i = 0\n",
    "for d in all_test_segments:\n",
    "    assert len(d[0])==15\n",
    "    c_acc_x = d[:, 0]\n",
    "    c_acc_y = d[:, 1]\n",
    "    c_acc_z = d[:, 2]\n",
    "    la_acc_x = d[:, 3]\n",
    "    la_acc_y = d[:, 4]\n",
    "    la_acc_z = d[:, 5]\n",
    "    la_gs_x = d[:, 6]\n",
    "    la_gs_y = d[:, 7]\n",
    "    la_gs_z = d[:, 8]\n",
    "    rla_acc_x = d[:, 9]\n",
    "    rla_acc_y = d[:, 10]\n",
    "    rla_acc_z = d[:, 11]\n",
    "    rla_gs_x = d[:, 12]\n",
    "    rla_gs_y = d[:, 13]\n",
    "    rla_gs_z = d[:, 14]\n",
    "    reading_list = [c_acc_x, c_acc_y, c_acc_z, la_acc_x, la_acc_y, la_acc_z, la_gs_x, la_gs_y, la_gs_z, rla_acc_x, rla_acc_y, rla_acc_z, rla_gs_x, rla_gs_y, rla_gs_z]\n",
    "    reading_name = [\"chest x-axis accelerometer\", \"chest y-axis accelerometer\", \"chest z-axis accelerometer\",\n",
    "                    \"left-ankle x-axis accelerometer\", \"left-ankle y-axis accelerometer\", \"left-ankle z-axis accelerometer\",\n",
    "                    \"left-ankle x-axis gyroscope\", \"left-ankle y-axis gyroscope\", \"left-ankle z-axis gyroscope\",\n",
    "                    \"right-lower-arm x-axis accelerometer\", \"right-lower-arm y-axis accelerometer\", \"right-lower-arm z-axis accelerometer\",\n",
    "                    \"right-lower-arm x-axis gyroscope\", \"right-lower-arm y-axis gyroscope\", \"right-lower-arm z-axis gyroscope\"]\n",
    "\n",
    "    data_dict = {\n",
    "        \"index\": i,\n",
    "        \"summaries\": {},\n",
    "        \"qa_pairs\": {name: [] for name in reading_name}\n",
    "    }\n",
    "\n",
    "    for r, n in zip(reading_list, reading_name):\n",
    "        normalized_n = \"normalized \" + n\n",
    "        t_df = analyze_trend(r, sr)\n",
    "        trend_dataframe = merge_adjacent_rows(t_df)\n",
    "\n",
    "        trend_pair_list = select_random_pair()\n",
    "\n",
    "        data_dict[\"summaries\"][n] = QA_summary(r, trend_dataframe, normalized_n, trend_pair_list,\n",
    "                                               whether_gpt=False, model_type='3.5')\n",
    "        data_dict[\"qa_pairs\"][n].append(dscb_simple_trend(trend_dataframe, normalized_n, trend_pair_list,\n",
    "                                                          whether_gpt=False, model_type='4'))\n",
    "    qa_dict[\"dataset\"].append(data_dict)\n",
    "    print(f\"{i} finished\")\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "with open(os.path.join(output_path, 'test', f\"mhealth_test_qa_stage1.json\"), 'w') as f:\n",
    "    json.dump(qa_dict, f, indent=2)\n",
    "print(len(qa_dict[\"dataset\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
